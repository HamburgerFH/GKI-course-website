[
  {
    "objectID": "include/99_altklausuren.html",
    "href": "include/99_altklausuren.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Nachfolgend finden Sie die Aufgabenstellung und die dazugehörige Musterlösung.\n\n\n\n\n\n\nFigure 1: Aufgabenstellung der Klausur vom 28.09.2024\n\n\n\n\n\n\n\n\n\nFigure 2: Lösung zur Klausur vom 28.09.2024",
    "crumbs": [
      "Altklausuren",
      "Aufgabentyp 1"
    ]
  },
  {
    "objectID": "include/99_altklausuren.html#klausuraufgaben-zu-sb-01",
    "href": "include/99_altklausuren.html#klausuraufgaben-zu-sb-01",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Nachfolgend finden Sie die Aufgabenstellung und die dazugehörige Musterlösung.\n\n\n\n\n\n\nFigure 1: Aufgabenstellung der Klausur vom 28.09.2024\n\n\n\n\n\n\n\n\n\nFigure 2: Lösung zur Klausur vom 28.09.2024",
    "crumbs": [
      "Altklausuren",
      "Aufgabentyp 1"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Teilgebiet der KI: Programme verbessern sich durch Erfahrung\nDefinition (Tom M. Mitchell, 1997):\n\nA computer program learns from experience \\(E\\) w. r. t. tasks \\(T\\) and performance \\(P\\)\nif its performance at \\(T\\), measured by \\(P\\), improves with \\(E\\)\n\nML-Algorithmen erkennen Muster in Daten und lernen daraus\nUnterschied zu klassischen Programmen: Lernen durch Rückkopplung\nhier könnte Abbildung 2.1 stehen\n\n\n\n\n\nTrainingsdaten \\(\\ra\\) Hypothese \\(\\ra\\) Ergebnis \\(\\ra\\) Feedback\nZiel: Optimierung der Hypothese durch Rückkopplung\nzwei Modi:\n\nOffline-Lernen: Training einmalig vor Anwendung\nOnline-Lernen: kontinuierliche Anpassung im Betrieb",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#maschinelles-lernen-themenfelder-und-überblick",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#maschinelles-lernen-themenfelder-und-überblick",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Teilgebiet der KI: Programme verbessern sich durch Erfahrung\nDefinition (Tom M. Mitchell, 1997):\n\nA computer program learns from experience \\(E\\) w. r. t. tasks \\(T\\) and performance \\(P\\)\nif its performance at \\(T\\), measured by \\(P\\), improves with \\(E\\)\n\nML-Algorithmen erkennen Muster in Daten und lernen daraus\nUnterschied zu klassischen Programmen: Lernen durch Rückkopplung\nhier könnte Abbildung 2.1 stehen\n\n\n\n\n\nTrainingsdaten \\(\\ra\\) Hypothese \\(\\ra\\) Ergebnis \\(\\ra\\) Feedback\nZiel: Optimierung der Hypothese durch Rückkopplung\nzwei Modi:\n\nOffline-Lernen: Training einmalig vor Anwendung\nOnline-Lernen: kontinuierliche Anpassung im Betrieb",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#ml-teilbereiche-und-abgrenzung",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#ml-teilbereiche-und-abgrenzung",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "ML: Teilbereiche und Abgrenzung",
    "text": "ML: Teilbereiche und Abgrenzung\n\n\nÜberblick\n\nUnsupervised Learning (UL): lernt aus nicht gelabelten Daten (z. B. Clustering)\nSupervised Learning (SL): lernt aus gelabelten Beispielen (z. B. Klassifikation, Regression)\nReinforcement Learning (RL): lernt durch Belohnung/Strafe in Interaktion mit Umwelt\n\n\n\nSupervised Learning (SL)\n\ngelabelte Trainingsdaten: \\((x_i, y_i)\\) bekannt für alle \\(i\\)\nZiel: Modell zur Vorhersage von \\(y\\) aus \\(x\\)\nzwei Problemtypen: Regression (z. B. Temperatur) und Klassifikation (z. B. medizinische Diagnose)\ntypische Anwendungen: Spracherkennung, Chatbots, Predictive Maintenance\ntypische Algorithmen: KNN, Random Forest, SVM, Neuronale Netze\n\n\n\nUnsupervised Learning (UL)\n\nkeine Zielvariable \\(y\\) – nur Merkmalsdaten \\(x\\)\nZiel: versteckte Muster oder Strukturen erkennen\nwichtigste Technik: Clustering\nBeispiele: Marktsegmente, soziale Gruppen, Themen in Texten\ntypische Algorithmen: K-Means, hierarchisches Clustering, spektrales Clustering\nAnwendung u. a. bei Empfehlungssystemen (z. B. Netflix)\n\n\n\nReinforcement Learning (RL)\n\nAgent lernt durch Interaktion mit einer Umgebung\nAktionen \\(a_t\\) verändern Zustand \\(s_t\\) und führen zu Reward \\(r_t\\)\nLernen basiert auf Trial & Error ohne vollständiges Umweltwissen\nZiel: Policy, die langfristig Belohnung maximiert\nzentrale Herausforderung: Exploration vs. Exploitation\ntypische Algorithmen: Q-Learning, SARSA, Monte Carlo, Temporal Difference\n\n\n\nTeilbereiche des ML (Überblick)\n\n\nACHTUNG: Auch diese Tabelle ist regelmäßig klausurrelevant",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#robotik",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#robotik",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Robotik",
    "text": "Robotik\n\nEinführung\n\nRobotik ist Teilbereich der KI, eng verbunden mit Maschinenbau und Informatik\nFokus: Interaktion mit physischer Welt über Sensoren und Aktoren\nzentrale Rolle in Industrie 4.0, z. B. durch fahrerlose Transportsysteme\ntypische Einsatzbereiche: automatisierte Produktion, Materialfluss, Mensch-Maschine-Kooperation\nzwei zentrale Robotertypen:\n\nCobots – kollaborative Roboter\nFTS/AMR – autonome Transportfahrzeuge\n\n\n\n\nCobots: kollaborative Roboter\n\nCobots arbeiten direkt mit Menschen ohne Schutzvorrichtung zusammen\nzentrale Idee: Kombination menschlicher Flexibilität mit robotischer Ausdauer\nvier Haupttypen nach ISO 10218:\n\nSicherheitsüberwachter Stopp\nGeschwindigkeit/Abstandsüberwachung\nLeistungs- und Kraftbegrenzung\nHandführung\n\n\n\n\nACHTUNG: Klausur (14.09.2024): bisher einmalig folgende Tabelle abgefragt:\n\n\n\nRobotik: Anwendungen kollaborativer Roboter\n\nPick & Place: Objekte greifen, bewegen, neu positionieren\nMaschinenversorgung: z. B. CNC-, Spritzgieß- oder Stanzmaschinen\nProzessunterstützung: Kleben, Bohren, Schweißen mit Endeffektor\nFertigstellung: Polieren, Schleifen, Entgraten mit Kraftsensor\nQualitätskontrolle: Bilderfassung & Sortierung mit maschinellem Sehen\nVerpackung: Entlastung bei monotonen Routineaufgaben\nGesundheitswesen:\n\nUnterstützung bei Hygiene, Vitaldaten, Blutabnahme\nautomatisierte Reha-Maßnahmen",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#autonome-transportsysteme",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#autonome-transportsysteme",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Autonome Transportsysteme",
    "text": "Autonome Transportsysteme\n\nEinführung: Fahrerlose Transportsysteme (FTS)\n\nselbstgesteuerte Fahrzeuge für Lager, Produktion, Logistik\nzwei Typen:\n\nAGV (automated guided vehicle): folgen Leitlinien (z. B. Magnetband)\nAMR (autonomous mobile robot): navigieren frei per Sensorik\n\nVorteile: effizienter Materialfluss, geringe Personalkosten, hohe Präzision\n\n\n\n\nACHTUNG: Klausur (14.09.2024): bisher einmalig folgende Tabelle abgefragt:\n\n\n\nUnterschiede AGV vs. AMR\n\nNavigation: AGV folgt externer Infrastruktur, AMR navigiert eigenständig\nHindernisse: AGV wird blockiert, AMR umfährt\nFlexibilität: AMR kann spontan neue Ziele anfahren\nKosten: AMR teurer, aber geringere Installationskosten\nSicherheit: AGV sicher bei klarer Spur, AMR benötigt Sicherheitsprotokolle",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#evolutionäre-algorithmen-genetische-verfahren",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#evolutionäre-algorithmen-genetische-verfahren",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Evolutionäre Algorithmen: genetische Verfahren",
    "text": "Evolutionäre Algorithmen: genetische Verfahren\n\nGenetische Algorithmen (GA): Grundprinzip\n\nheuristische Optimierung basierend auf Darwin’scher Selektion\nLösung = Chromosom; Variable = Gen\nZiel: mit Selektion, Crossover und Mutation bessere Lösungen erzeugen\nFitnessfunktion bewertet Qualität jeder Lösung\n\n\n\n\nOperatoren: Selektion, Crossover, Mutation\n\nSelektion: fittere Individuen haben höhere Auswahlwahrscheinlichkeit\nCrossover: Austausch von Genen zur Kombination guter Eigenschaften\nMutation: zufällige Genänderung zur Vermeidung lokaler Minima\nElitismus: beste Lösungen direkt in nächste Generation übernommen\n\n\n\n\nACHTUNG: Klausurrelevant mehrmals – Definitionen\nBeachte die jeweiligen einzeln hervorgehoen Sätze !\n\nKreuzung (Crossover):\nDie natürliche Selektion ermöglicht die Auswahl von Individuen als Eltern für den Crossover-Schritt. Dieser Schritt ermöglicht den Austausch von Genen zwischen Individuen, um neue Lösungen zu erzeugen. In der Literatur gibt es verschiedene Crossover-Methoden. Bei der einfachsten Methode werden die Chromosomen an zwei oder drei Stellen geteilt. Anschließend werden dann die Gene zwischen zwei Chromosomen ausgetauscht.\nMutation:\nMit Hilfe der Mutation werden zufällige Veränderungen in den Genen erzeugt. Es gibt einen Parameter namens Mutationswahrscheinlichkeit \\((P_m)\\), der für jedes Gen in einem Kinderchromosom verwendet wird, das in der Crossover-Phase erzeugt wird. Dieser Parameter ist eine Zahl im Intervall von \\([0,1]\\). Für jedes Gen im neuen Kindchromosom wird eine Zufallszahl im gleichen Intervall erzeugt. Wenn diese Zufallszahl kleiner als \\(P_m\\) ist, wird dem Gen eine Zufallszahl mit der unteren und oberen Grenze zugewiesen.",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#schwarmintelligenz",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#schwarmintelligenz",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Schwarmintelligenz",
    "text": "Schwarmintelligenz\n\nGrundprinzip\n\nkollektives Verhalten einfacher Agenten (z. B. Insekten, Vögel)\nSystemverhalten entsteht durch lokale Interaktion & Selbstorganisation\nkein zentrales Steuerungselement, aber globale Zielerreichung möglich\nrobust gegenüber Ausfällen, sehr flexibel einsetzbar\n\n\n\nAmeisenalgorithmus (Ant Colony Optimization, ACO)\n\nVorbild: reale Ameisen suchen effizient kürzeste Wege\nKommunikation über Pheromonspuren – Stigmergie\nBestandteile:\n\nkünstliche Ameisen als Lösungskonstrukte\nPheromon-Aktualisierung (Verstärkung & Verdunstung)\noptional: Daemon-Strategien zur Sicherung guter Lösungen\n\ngeeignet für kombinatorische Optimierung (z. B. Routing-Probleme)",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_02_themenfelder_kuenstliche_intelligenz.html#expertensysteme-es-und-multiagentensysteme-mas",
    "href": "include/01_02_themenfelder_kuenstliche_intelligenz.html#expertensysteme-es-und-multiagentensysteme-mas",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Expertensysteme (ES) und Multiagentensysteme (MAS)",
    "text": "Expertensysteme (ES) und Multiagentensysteme (MAS)\n\nAufbau und Arbeitsweise\n\nwissensbasierte Systeme: simulieren Expertenwissen\nbestehen aus:\n\nWissensbasis: Fakten, Regeln, Heuristiken\nInferenzmaschine: Schlussfolgerung durch Verkettung\nBenutzeroberfläche: Eingabe/Interaktion\n\nVorwärtsverkettung: datengetriebene Ableitung\nRückwärtsverkettung: hypothesengeleitete Prüfung\n\n\n\n\nEigenschaften und Herausforderungen\n\nNetzwerk autonomer Agenten zur Lösung komplexer Probleme\nzentrale Merkmale:\n\nSituiertheit, Autonomie, Inferenzfähigkeit, Reaktivität\nProaktivität, soziales Verhalten\n\nVorteile: Robustheit, Skalierbarkeit, Wiederverwendbarkeit\nHerausforderungen: Koordination, Abstraktion, Konfliktlösung",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Themenfelder der Künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html",
    "href": "include/02_03_SLP.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Binäre Klassifikation mit \\(d\\)-dimensionaler Eingabe/Input \\(\\bs{x} \\in \\mathbb{R}^d\\):\n\\[\nf: \\mathbb{R}^d \\ra \\{0,1\\}\\\\\n\\bs{x} \\mapsto f(x)=\\hat{y}\n\\]\nAusgabe: \\(\\hat{y} = \\phi(\\bs{w}^\\top \\bs{x} + b)\\) mit Gewichtungsvektor \\(\\bs{w} \\in \\mathbb{R}^d\\) und Bias \\(b \\in \\mathbb{R}\\)\nAktivierungsfunktion: für binäre Klassifikation typisch – Heaviside:\n\\[\n\\phi(z) =\n  \\begin{cases}\n    1 & \\text{wenn } z \\geq 0 \\\\\n    0 & \\text{wenn } z &lt; 0\n  \\end{cases}\n\\]\nLinearkombination: \\(z = \\bs{w}^\\top \\bs{x} + b\\) stellt lineare Entscheidungsgrenze dar mit \\(\\bs{w}^\\top \\bs{x} + b = 0\\) und entsprechender Vorhersage \\(\\hat{y} = \\phi(z)\\) –\nBeispiel: \\(d = 2\\), \\(\\bs{x}_i = \\begin{pmatrix} x_{i1} \\\\ x_{i2} \\end{pmatrix}\\),\n\\(\\bs{w} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\), \\(b = -1\\).\nEntscheidungsgrenze ist dann:\n\\[\n2 x_{i1} - x_{i2} - 1 = 0 \\quad \\Leftrightarrow \\quad x_{i2} = 2 x_{i1} - 1\n\\]\nPunkte oberhalb der Geraden \\(\\ra\\) Klasse 1, unterhalb \\(\\ra\\) Klasse 0\nBedingung: \\(\\bs{w}^T \\bs{x}_i + b = 0\\) trennt den Eingaberaum in zwei Halbräume:\n\nPunkte mit \\(\\bs{w}^T \\bs{x}_i + b \\geq 0\\) \\(\\ra\\) Klasse 1\nPunkte mit \\(\\bs{w}^T \\bs{x}_i + b &lt; 0\\) \\(\\ra\\) Klasse 0",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#single-layer-perzeptron-slp",
    "href": "include/02_03_SLP.html#single-layer-perzeptron-slp",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Binäre Klassifikation mit \\(d\\)-dimensionaler Eingabe/Input \\(\\bs{x} \\in \\mathbb{R}^d\\):\n\\[\nf: \\mathbb{R}^d \\ra \\{0,1\\}\\\\\n\\bs{x} \\mapsto f(x)=\\hat{y}\n\\]\nAusgabe: \\(\\hat{y} = \\phi(\\bs{w}^\\top \\bs{x} + b)\\) mit Gewichtungsvektor \\(\\bs{w} \\in \\mathbb{R}^d\\) und Bias \\(b \\in \\mathbb{R}\\)\nAktivierungsfunktion: für binäre Klassifikation typisch – Heaviside:\n\\[\n\\phi(z) =\n  \\begin{cases}\n    1 & \\text{wenn } z \\geq 0 \\\\\n    0 & \\text{wenn } z &lt; 0\n  \\end{cases}\n\\]\nLinearkombination: \\(z = \\bs{w}^\\top \\bs{x} + b\\) stellt lineare Entscheidungsgrenze dar mit \\(\\bs{w}^\\top \\bs{x} + b = 0\\) und entsprechender Vorhersage \\(\\hat{y} = \\phi(z)\\) –\nBeispiel: \\(d = 2\\), \\(\\bs{x}_i = \\begin{pmatrix} x_{i1} \\\\ x_{i2} \\end{pmatrix}\\),\n\\(\\bs{w} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\), \\(b = -1\\).\nEntscheidungsgrenze ist dann:\n\\[\n2 x_{i1} - x_{i2} - 1 = 0 \\quad \\Leftrightarrow \\quad x_{i2} = 2 x_{i1} - 1\n\\]\nPunkte oberhalb der Geraden \\(\\ra\\) Klasse 1, unterhalb \\(\\ra\\) Klasse 0\nBedingung: \\(\\bs{w}^T \\bs{x}_i + b = 0\\) trennt den Eingaberaum in zwei Halbräume:\n\nPunkte mit \\(\\bs{w}^T \\bs{x}_i + b \\geq 0\\) \\(\\ra\\) Klasse 1\nPunkte mit \\(\\bs{w}^T \\bs{x}_i + b &lt; 0\\) \\(\\ra\\) Klasse 0",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#lernen-von-parametern-im-slp",
    "href": "include/02_03_SLP.html#lernen-von-parametern-im-slp",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Lernen von Parametern im SLP",
    "text": "Lernen von Parametern im SLP\n\nIntuition\n\nTrainingsdaten: \\((\\bs{x}_i, y_i)\\) mit Label \\(y_i \\in \\{0,1\\}\\), \\(\\bs{x}_i \\in \\mathbb{R}^d\\) für \\(i = 1, \\dots, N\\), Vorhersage \\(\\hat{y}_i = \\phi(\\bs{w}^\\top \\bs{x}_i + b)\\), Gewichtungsvektor \\(\\bs{w} \\in \\mathbb{R}^d\\), und \\(b \\in \\mathbb{R}\\)\nZiel: Bestimme \\(\\bs{w}\\) und \\(b\\), sodass \\(\\hat{y}_i \\approx y_i\\) für alle \\(i\\),\n\n\nbei Fehlklassifikation \\(y_i \\ne \\hat{y}_i\\): Update der Lernregel:\n\n\n\\[\n\\bs{w} \\leftarrow \\bs{w} + \\eta (y_i - \\hat{y}_i) \\bs{x}_i, \\quad\nb \\leftarrow b + \\eta (y_i - \\hat{y}_i)\n\\]\nLernrate (Schrittweite) \\(\\eta&gt;0\\), \\(\\bs{w}\\) und \\(b\\) über Korrektursignal \\(y_i - \\hat{y}_i \\in \\{-1, 0, +1\\}\\) bei Fehler anpassen: inkrementell\nkorrekt klassifizierte Punkte bewirken nichts\n\n\n\nBedeutung des Bias-Terms \\(b\\)\n\n\\(b\\) kann als zusätzliches Gewicht \\(w_0\\) mit festem Input \\(x_0 = 1\\) interpretiert werden \\(\\ra\\) das Modell wird zu:\n\\[\n\\hat{y} = \\phi(\\bs{w}^\\top \\bs{x} + b) = \\phi(\\tilde{\\bs{w}}^\\top \\tilde{\\bs{x}})\n\\]\n\\[\n\\tilde{\\bs{x}} = \\begin{pmatrix} 1 \\\\ \\bs{x} \\end{pmatrix}, \\quad\n\\tilde{\\bs{w}} = \\begin{pmatrix} b \\\\ \\bs{w} \\end{pmatrix}\n\\]\nOhne Bias: Entscheidungshyperfläche \\(\\bs{w}^\\top \\bs{x} = 0\\) verläuft durch den Ursprung\nMit Bias \\(b \\in \\mathbb{R}\\): Verschiebung der Hyperfläche auf \\(\\bs{w}^\\top \\bs{x} + b = 0\\)\nBias wirkt wie ein frei lernbarer Achsenabschnitt, was verschiebbare Aktivierungsschwellen ermöglicht",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#lernen-von-parametern",
    "href": "include/02_03_SLP.html#lernen-von-parametern",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Lernen von Parametern",
    "text": "Lernen von Parametern\n\nOnline-Lernen und Gewichtsanpassung\n\nGewichtsupdate erfolgt sofort nach Verarbeitung von \\((\\bs{x}_i, y_i)\\)\nDieser Lernmodus heißt online: kein Warten auf ganzen Batch\nNur Fehlerpunkte (\\(y_i \\ne \\hat{y}_i\\)) führen zu einer Änderung \\(\\ra\\) korrekt klassifizierte Punkte lassen \\(\\bs{w}\\) und \\(b\\) unverändert\nNetzparameter \\(\\bs{w}\\), \\(b\\) sind global – gelten für alle Beobachtungen \\(\\ra\\) Änderung an \\(\\bs{w}\\) durch Punkt \\(x_k\\) kann spätere \\(\\hat{y}_j\\), \\(k&lt;j\\), beeinflussen\nDaher kann ein zuvor korrekt klassifiziertes Trainingsdatum falsch werden\n\n\n\nEinschränkungen des Perzeptrons\n\nKonvergenz garantiert, falls Daten linear separierbar sind\nBeispiel: XOR-Funktion nicht trennbar durch eine lineare Hyperebene\nUrsache: keine nichtlineare Transformation des Eingaberaums möglich\nLösungsidee: Zusammenschalten mehrerer Neuronen\n\\(\\hra\\) mehrschichtige Netzwerke (MLP)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#was-ist-ein-neuron",
    "href": "include/02_03_SLP.html#was-ist-ein-neuron",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Was ist ein Neuron?",
    "text": "Was ist ein Neuron?\n\nAbgrenzung zur Trainingsbeobachtung\n\nTrainingsdaten: \\(N\\) Vektoren \\(\\bs{x}_i \\in \\mathbb{R}^d\\) mit zugehörigem Label \\(y_i \\in \\{0,1\\}\\)\n\\(\\bs{x}_i\\) besitzt \\(d\\) Merkmale \\(\\ra\\) Eingabeschicht hat \\(d\\) Inputneuronen\nEin Neuron ist eine Verarbeitungseinheit im Netzwerk, nicht ein Trainingsdatum\n\\(\\ra\\) nicht \\(N\\) Neuronen, sondern \\(N\\) Durchläufe durch dasselbe Netzwerk\nIm Single-Layer-Perzeptron: genau ein Outputneuron in der Ausgabeschicht\n\\(\\ra\\) nimmt alle \\(d\\) Komponenten von \\(\\bs{x}\\) auf:\n\nberechnet gewichtete Summe \\(z = \\bs{w}^\\top \\bs{x} + b\\)\nentscheidet per Schwellenfunktion \\(\\phi(z)\\), ob Output \\(0\\) oder \\(1\\)\npro Eingabevektor entsteht genau ein Ausgabewert \\(\\hat{y} \\in \\{0,1\\}\\)\n\n\n\n\nWiederholtes Durchlaufen der Daten\n\nTrainingsdaten \\(\\{(\\bs{x}_i, y_i)\\}_{i=1}^N\\) werden mehrfach durchlaufen\n\\(\\hra\\) jeder vollständige Durchlauf heißt Epoche\nPro Epoche: alle \\(\\bs{x}_i\\) werden sequentiell verarbeitet:\nfor-loop über \\(i = 1,\\dots,N\\) für jedes \\(i\\) Vorhersage \\(\\hat{y}_i\\), ggf. Update\nReihenfolge der Daten bleibt konstant oder wird pro Epoche neu gemischt",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#ziel-der-perzeptron-lernregel-formale-ableitung-perzeptron-konvergenzsatz",
    "href": "include/02_03_SLP.html#ziel-der-perzeptron-lernregel-formale-ableitung-perzeptron-konvergenzsatz",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Ziel der Perzeptron-Lernregel, Formale Ableitung & Perzeptron-Konvergenzsatz",
    "text": "Ziel der Perzeptron-Lernregel, Formale Ableitung & Perzeptron-Konvergenzsatz\n\nZiel der Perzeptron-Lernregel\n\nTrainingsdaten: \\((\\bs{x}_i, y_i)\\) mit \\(\\bs{x}_i \\in \\mathbb{R}^d\\), \\(y_i \\in \\{0,1\\}\\), \\(i = 1,\\dots,N\\)\nModell:\n\\[\n\\hat{y}_i = \\phi\\left( \\bs{w}^\\top \\bs{x}_i + b \\right)\n\\]\nFrage: wie lernen wir \\(\\bs{w}\\) und \\(b\\) aus den Daten?\nFalls \\(\\hat{y}_i \\neq y_i\\), aktualisiere:\n\\[\n\\bs{w} \\leftarrow \\bs{w} + \\eta (y_i - \\hat{y}_i) \\bs{x}_i\n\\] \\[\nb \\leftarrow b + \\eta (y_i - \\hat{y}_i)\n\\]\n\\(\\eta &gt; 0\\): Lernrate,\n\\(y_i - \\hat{y}_i \\in \\{-1, 0, +1\\}\\): Korrektursignal\nIntuition:\n\n\\(y_i = 1\\), \\(\\hat{y}_i = 0\\) \\(\\ra\\) schiebe Grenze nach rechts\n\\(y_i = 0\\), \\(\\hat{y}_i = 1\\) \\(\\ra\\) schiebe Grenze nach links\nbei \\(\\hat{y}_i = y_i\\) keine Änderung\n\n\n\n\nFormale Ableitung\n\nDer Fehler bei Datum/Sample \\(i\\) sei: \\(e_i = y_i - \\hat{y}_i\\)\nDie Gewichtsanpassung erfolgt entlang des negativen Gradienten (heuristisch), da die Heaviside-Funktion nicht differenzierbar ist. Dennoch interpretiert man die Lernregel grob als ob sie dem Gradienten von \\(e_i^2\\) folgt:\n\\[\n\\Delta \\bs{w} \\propto \\frac{\\partial}{\\partial \\bs{w}} e_i^2\n= -2 e_i \\frac{\\partial \\hat{y}_i}{\\partial \\bs{w}} \\approx e_i \\bs{x}_i\n\\]\n\\[\n\\nabla_{\\bs{w}} e_i^2 = -2 e_i \\cdot \\frac{\\partial \\hat{y}_i}{\\partial \\bs{w}}\n\\approx e_i \\cdot \\bs{x}_i\n\\]\nNäherung: Schritt / Richtung der Korrektur ist \\(\\bs{x}_i\\) (gewichtet mit Vorzeichen des Fehlers), da\n\\(\\frac{\\partial \\hat{y}_i}{\\partial \\bs{w}}\\) bei Heaviside nicht definiert\n\n\n\nPerzeptron-Konvergenzsatz\n\nTheorem: falls Daten linear separierbar, konvergiert Lernregel\nNach endlich vielen Schritten existiert \\(\\bs{w}^*\\) mit \\(\\hat{y}_i = y_i\\) für alle \\(i\\)\nBeweisidee (nicht im Detail):\n\nkonstruiere Maß für Fortschritt (z. B. Projektion auf wahre Trennrichtung)\nzeige monotonen Fortschritt bei Fehlklassifikation\nzeige, dass keine unendliche Zahl an Fehlern möglich ist\n\nAber Bei nicht separierbaren Daten \\(\\ra\\) keine Konvergenz, endlose Oszillation möglich\n\n\n\nGrenzen der Lernregel\n\nFunktioniert nur bei linear separierbaren Daten\nKeine Interpretation als Gradientenabstieg auf differenzierbarer Funktion\nGrund: Sprungstelle der Heaviside-Funktion\nBei nicht separierbaren Daten: Oszillation ohne Konvergenz\n\n\n\nBeispiel: ein Lernschritt\n\nGegeben: \\(\\bs{x}_1 = (1,1)^T\\), \\(y_1 = 1\\), Initialisierung:\n\\[\n\\bs{w} = (0,0)^T,\\quad b = 0,\\quad \\eta = 1\n\\]\nDann: \\(z_1 = 0 \\Rightarrow \\hat{y}_1 = 1 \\Rightarrow\\) keine Änderung\nWeiteres Sample: \\(\\bs{x}_2 = (-2, 0)^T\\), \\(y_2 = 0\\)\nDann: \\(z_2 = 0 \\Rightarrow \\hat{y}_2 = 1 \\Rightarrow\\) Fehler \\(e_2 = -1\\)\nUpdate:\n\\[\n\\bs{w} \\leftarrow (0,0) -1 \\cdot (-2,0) = (2,0),\\quad b \\leftarrow 0 -1 = -1\n\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#differenzierbares-perzeptron-und-gradientenverfahren",
    "href": "include/02_03_SLP.html#differenzierbares-perzeptron-und-gradientenverfahren",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Differenzierbares Perzeptron und Gradientenverfahren",
    "text": "Differenzierbares Perzeptron und Gradientenverfahren\n\nMotivation für Differenzierbarkeit\n\nKlassische Heaviside-Aktivierung ist nicht differenzierbar\nGradientverfahren wie SGD daher nicht direkt anwendbar\nLösung: Ersetze Heaviside durch glatte Approximation, z. B. Sigmoid\n\n\n\nSigmoid-Aktivierung\n\nDefinition:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad z \\in \\mathbb{R}\n\\]\nEigenschaften:\n\nglatt und überall differenzierbar\nWertebereich: \\((0,1)\\)\nSättigung bei großen positiven/negativen \\(z\\)\nAbleitung:\n\\[\n\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n\\]\n\n\n\n\nModell mit Sigmoid-Aktivierung\n\nFür \\(i = 1, \\dots, N\\):\n\\[\nz_i = \\bs{w}^\\top \\bs{x}_i + b,\\quad \\hat{y}_i = \\sigma(z_i)\n\\]\nZiel: \\(\\hat{y}_i \\approx y_i \\in \\{0,1\\}\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_03_SLP.html#differenzierbares-perzeptron-und-gradientenverfahren-1",
    "href": "include/02_03_SLP.html#differenzierbares-perzeptron-und-gradientenverfahren-1",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Differenzierbares Perzeptron und Gradientenverfahren",
    "text": "Differenzierbares Perzeptron und Gradientenverfahren\n\nFehlerfunktion: log loss\n\nInterpretation von \\(\\hat{y}_i\\) als Wahrscheinlichkeit \\(\\Rightarrow\\) log loss:\n\\[\n\\mathcal{L}_i = - \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\nGesamte Verlustfunktion:\n\\[\n\\mathcal{L} = \\sum_{i=1}^N \\mathcal{L}_i\n= - \\sum_{i=1}^N \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\n\n\n\nGradienten für Gewichte und Bias\n\nAbleitung nach \\(\\bs{w}\\):\n\\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial \\bs{w}} = (\\hat{y}_i - y_i) \\cdot \\bs{x}_i\n\\]\nAbleitung nach \\(b\\):\n\\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial b} = \\hat{y}_i - y_i\n\\]\n\n\n\nUpdate-Regeln (Gradient Descent)\n\nBatch-Update für Lernrate \\(\\eta &gt; 0\\):\n\\[\n\\bs{w} \\leftarrow \\bs{w} - \\eta \\sum_{i=1}^N (\\hat{y}_i - y_i) \\bs{x}_i\n\\] \\[\nb \\leftarrow b - \\eta \\sum_{i=1}^N (\\hat{y}_i - y_i)\n\\]\nStochastisch (SGD):\n\\[\n\\bs{w} \\leftarrow \\bs{w} - \\eta (\\hat{y}_i - y_i) \\bs{x}_i,\\quad\nb \\leftarrow b - \\eta (\\hat{y}_i - y_i)\n\\]\n\n\n\nInterpretation\n\nFehlerterm \\(\\hat{y}_i - y_i\\) misst Abstand zur Zielklasse\nRichtung der Korrektur: Eingabevektor \\(\\bs{x}_i\\)\nDurch Differenzierbarkeit der Sigmoid-Funktion \\(\\ra\\) Grundlage für Backpropagation",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze I: SLP"
    ]
  },
  {
    "objectID": "include/02_01_grundlagen_und_definitionen.html",
    "href": "include/02_01_grundlagen_und_definitionen.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Künstliche Intelligenz umfasst eine Vielzahl von Algorithmen zur automatisierten Datenverarbeitung\nEntscheidungsbäume gehören zum Bereich des maschinellen Lernens\nFokus dieser Vorlesung: Methoden des überwachten Lernens\nmaschinelles Lernen ist ein Teilbereich der KI, bei dem Systeme aus Daten lernen statt explizit programmiert zu werden\n\n\n\n\n\nein zentral gesteuerter Algorithmus verwaltet Daten, Parameter und Logik an einem einzigen Ort\nVorteil: einfache Struktur, leicht zu implementieren\nNachteil: zentraler Ausfallpunkt, eingeschränkte Skalierbarkeit\nNachteil: zentraler Ausfallpunkt, schlecht skalierbar bei verteilten Aufgaben\nBeispiel: ineffizient zur Steuerung ganzer Fahrzeugflotten\nzentrale Modelle wie Entscheidungsbäume folgen einer klaren, regelbasierten Struktur und sind gut interpretierbar\n\n\n\n\n\nZiel: aus Eingabevariablen \\(X\\) eine Vorhersage \\(Y\\) oder \\(G\\) ableiten\nauch möglich: geordnete Kategorien (z. B. klein, mittel, groß) – hier nicht behandelt\nRegression: \\(Y\\) ist quantitativ, z. B. Preis, Temperatur, Gewicht\nKlassifikation: \\(G\\) ist qualitativ, z. B. Klasse Ja/Nein'',rot/blau’’\n``überwacht’’ bedeutet: Trainingsdaten enthalten Zielgrößen zur Kontrolle des Lernprozesses\nbeide Aufgaben lassen sich als Funktionsapproximation formulieren\nTrainingsdaten bestehen aus Paaren \\((x_i, y_i)\\) oder \\((x_i, g_i)\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Grundlagen und Definitionen"
    ]
  },
  {
    "objectID": "include/02_01_grundlagen_und_definitionen.html#grundlagen-der-künstlichen-intelligenz",
    "href": "include/02_01_grundlagen_und_definitionen.html#grundlagen-der-künstlichen-intelligenz",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Künstliche Intelligenz umfasst eine Vielzahl von Algorithmen zur automatisierten Datenverarbeitung\nEntscheidungsbäume gehören zum Bereich des maschinellen Lernens\nFokus dieser Vorlesung: Methoden des überwachten Lernens\nmaschinelles Lernen ist ein Teilbereich der KI, bei dem Systeme aus Daten lernen statt explizit programmiert zu werden\n\n\n\n\n\nein zentral gesteuerter Algorithmus verwaltet Daten, Parameter und Logik an einem einzigen Ort\nVorteil: einfache Struktur, leicht zu implementieren\nNachteil: zentraler Ausfallpunkt, eingeschränkte Skalierbarkeit\nNachteil: zentraler Ausfallpunkt, schlecht skalierbar bei verteilten Aufgaben\nBeispiel: ineffizient zur Steuerung ganzer Fahrzeugflotten\nzentrale Modelle wie Entscheidungsbäume folgen einer klaren, regelbasierten Struktur und sind gut interpretierbar\n\n\n\n\n\nZiel: aus Eingabevariablen \\(X\\) eine Vorhersage \\(Y\\) oder \\(G\\) ableiten\nauch möglich: geordnete Kategorien (z. B. klein, mittel, groß) – hier nicht behandelt\nRegression: \\(Y\\) ist quantitativ, z. B. Preis, Temperatur, Gewicht\nKlassifikation: \\(G\\) ist qualitativ, z. B. Klasse Ja/Nein'',rot/blau’’\n``überwacht’’ bedeutet: Trainingsdaten enthalten Zielgrößen zur Kontrolle des Lernprozesses\nbeide Aufgaben lassen sich als Funktionsapproximation formulieren\nTrainingsdaten bestehen aus Paaren \\((x_i, y_i)\\) oder \\((x_i, g_i)\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Grundlagen und Definitionen"
    ]
  },
  {
    "objectID": "include/02_01_grundlagen_und_definitionen.html#beispiel-klassifikation-mit-iris-daten",
    "href": "include/02_01_grundlagen_und_definitionen.html#beispiel-klassifikation-mit-iris-daten",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Klassifikation mit Iris-Daten",
    "text": "Beispiel: Klassifikation mit Iris-Daten\n\n\n\n\n\n\nFigure 1: Abbildung\n\n\n\n\nEingabe: Länge und Breite von Kelch- und Blütenblättern\nSepal = Kelchblatt vs. Petal = Blütenblatt (siehe Bild)\nZiel: Vorhersage der Iris-Spezies (setosa, versicolor, virginica)\ntypische Klassifikationsaufgabe mit \\(G \\in \\{1, 2, 3\\}\\)\n\n\nHintergrund zum Datensatz\n\nstammt aus einer Studie von Ronald Fisher (1936)\nenthält 150 Beobachtungen, je 50 pro Iris-Art: setosa, versicolor, virginica\nfür jede Pflanze sind 4 Merkmale gegeben: Kelchblatt-Länge/Breite und Blütenblatt-Länge/Breite\nseit Jahrzehnten Standard-Datensatz zum Testen von Klassifikationsverfahren\ngut geeignet für visuelle Trennung & erste Modellbeispiele",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Grundlagen und Definitionen"
    ]
  },
  {
    "objectID": "include/02_01_grundlagen_und_definitionen.html#notation",
    "href": "include/02_01_grundlagen_und_definitionen.html#notation",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Notation",
    "text": "Notation\n\nEingabedaten (Input)\n\n\\(\\bs{X} \\in \\mathbb{R}^{N \\times p}\\): Matrix aller Inputvektoren und eine Input/Eingabe: \\(\\bs{x}_i \\in \\mathbb{R}^p\\), \\(i = 1, \\dots, N\\) Beobachtungen\ndazugehöriger Zeilenvektor: \\(\\bs{x}_i^T\\) (transponierter Spaltenvektor) der \\(i\\)-ten Beobachtung\n\n\\[\n\\bs{X} =\n\\begin{bmatrix}\n  x_{11} & x_{12} & \\dots & x_{1p} \\\\\\\\\n  \\vdots & \\vdots &       & \\vdots \\\\\\\\\n  x_{i1} & x_{i2} & \\dots & x_{ip} \\\\\\\\\n  \\vdots & \\vdots &       & \\vdots \\\\\\\\\n  x_{N1} & x_{N2} & \\dots & x_{Np}\n\\end{bmatrix}\n\\quad \\Rightarrow \\quad \\bs{x}_i^T = \\text{i-te Zeile}\n\\]\n\nfür den Iris Datensatz wäre \\(p=4\\), \\(N=150\\) und die Reihenfolge\n\\(\\bs{x}_i^T\\) enthält alle vier Merkmale der \\(i\\)-ten Blüte: sepal_length sepal_width petal_length petal_width\nBeispiel: \\(x_{i2}\\) = sepal-Breite der \\(i\\)-ten Blüte, und \\(x_{N3}\\) = petal-Länge der \\(N\\)-ten Blüte\n\n\n\nZielgrößen (Output)\n\n\\(Y_i \\in \\mathbb{R}\\): quantitatives Ziel (Regression)\n\\(G_i \\in \\Omega\\): qualitatives Ziel (Klassifikation), z. B. \\(\\Omega = \\{\\text{setosa}, \\dots\\}\\)\n\\(\\hat{Y}_i\\), \\(\\hat{G}_i\\): geschätzte Ausgaben\nZiel: \\(\\hat{Y}_i \\approx Y_i\\), \\(\\hat{G}_i \\approx G_i\\)\nBinäre Klassifikation für binäres \\(G\\): \\(\\hat{Y}_i \\in [0, 1]\\) und Schwellenwert-Regel z. B. für Wert 0.5:\n\n\\[\n\\hat{G}_i =\n\\begin{cases}\n  1 & \\text{falls } \\hat{Y}_i \\geq 0{,}5 \\\\\\\\\n  0 & \\text{falls } \\hat{Y}_i &lt; 0{,}5\n\\end{cases}\n\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Grundlagen und Definitionen"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html",
    "href": "include/02_02_entscheidungsbaeume.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Struktur zur Klassifikation oder Regression basierend auf Tests auf Attributwerten \\(X_j\\) \\(\\Rightarrow\\) typische Anwendungen:\n\nmedizinische Diagnose\nKreditwürdigkeitsprüfung\nZielgruppenklassifikation im Marketing\n\nZiel: herleitung einer regel, die Objekte korrekt einer Klasse \\(K_m\\) zuordnet\njeder Pfad: Folge von Tests \\(T_1, T_2, \\dots\\) auf Attribute; daraus entstehen Zweige, die zu einem Blatt führen\nformale Sicht: Entscheidungsbaum als Funktion zum Beispiel: \\(f : \\mathcal{X} \\rightarrow \\mathcal{Y}\\) \\[\\begin{align*}\nf:~ \\mathbb{R}^p &\\rightarrow \\{1, \\dots, K\\}\\\\\n\\bs{X} &\\mapsto K_m\n\\end{align*}\\]\nDurch sukzessive Anwendung der Tests \\(T_1, T_2,\\ldots\\) erhalten wir eine Aufteilung des Merkmalsraums \\(\\mathcal{X}\\) in disjunkte Regionen \\(R_1, \\dots, R_M\\) und somit am Ende \\(f(\\bs{X}) = K_m\\) für potentielle komplett neue Objekte\njedes Objekt wird eindeutig einem Blatt mit Klassenlabel zugeordnet\nWurzelknoten: Startpunkt der Klassifikationsregel\ninterner Knoten: Test auf Attributwert, z. B. \\(X_2 &lt; 0.53\\)?\nZweig: Ausprägung des Testergebnisses (z. B. „ja“ oder „nein“)\nBlattknoten: Ergebnis der Klassifikation",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#entscheidungsbäume-grundidee",
    "href": "include/02_02_entscheidungsbaeume.html#entscheidungsbäume-grundidee",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Struktur zur Klassifikation oder Regression basierend auf Tests auf Attributwerten \\(X_j\\) \\(\\Rightarrow\\) typische Anwendungen:\n\nmedizinische Diagnose\nKreditwürdigkeitsprüfung\nZielgruppenklassifikation im Marketing\n\nZiel: herleitung einer regel, die Objekte korrekt einer Klasse \\(K_m\\) zuordnet\njeder Pfad: Folge von Tests \\(T_1, T_2, \\dots\\) auf Attribute; daraus entstehen Zweige, die zu einem Blatt führen\nformale Sicht: Entscheidungsbaum als Funktion zum Beispiel: \\(f : \\mathcal{X} \\rightarrow \\mathcal{Y}\\) \\[\\begin{align*}\nf:~ \\mathbb{R}^p &\\rightarrow \\{1, \\dots, K\\}\\\\\n\\bs{X} &\\mapsto K_m\n\\end{align*}\\]\nDurch sukzessive Anwendung der Tests \\(T_1, T_2,\\ldots\\) erhalten wir eine Aufteilung des Merkmalsraums \\(\\mathcal{X}\\) in disjunkte Regionen \\(R_1, \\dots, R_M\\) und somit am Ende \\(f(\\bs{X}) = K_m\\) für potentielle komplett neue Objekte\njedes Objekt wird eindeutig einem Blatt mit Klassenlabel zugeordnet\nWurzelknoten: Startpunkt der Klassifikationsregel\ninterner Knoten: Test auf Attributwert, z. B. \\(X_2 &lt; 0.53\\)?\nZweig: Ausprägung des Testergebnisses (z. B. „ja“ oder „nein“)\nBlattknoten: Ergebnis der Klassifikation",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-entscheidungsbaum",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-entscheidungsbaum",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Entscheidungsbaum",
    "text": "Beispiel: Entscheidungsbaum\n\n\nrote Punkte: Klasse sick'' vs. grüne Punkte: Klassehealthy’’\nhorizontale und vertikale Linien: durch Entscheidungsbaum induzierte Schwellenwerte\nBeispielpfad: \\(X_2 &lt; 0.53\\) und \\(X_1 &lt; 0{,11}\\) führt zur Klasse „sick“\njeder Pfad im Baum definiert eine Region im Merkmalsraum\nZiel: vollständige Trennung der Klassen durch einfache Tests auf Attributwerte",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#induktionsaufgabe-und-generalisierbarkeit",
    "href": "include/02_02_entscheidungsbaeume.html#induktionsaufgabe-und-generalisierbarkeit",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Induktionsaufgabe und Generalisierbarkeit",
    "text": "Induktionsaufgabe und Generalisierbarkeit\n\nZiel: finde Klassifikationsregel \\(f: \\mathcal{X} \\to \\mathcal{Y}\\), die neue Objekte korrekt einer Klasse zuordnet\ngegeben: Trainingsmenge \\(M = \\{(x_i, y_i)\\}_{i=1}^N\\) mit Attributen \\(X_1, \\dots, X_d\\)\ngesucht: Entscheidungsbaum, der \\(M\\) korrekt trennt und auf ungesehene Daten generalisiert\nVoraussetzung: Attribute müssen Klassen unterscheidbar machen\nfalls \\(x_i = x_j\\), aber \\(y_i \\neq y_j\\): keine trennbare Klassifikation möglich\nZiel ist nicht nur perfekte Trennung von \\(M\\), sondern robuste Generalisierung\nbei mehreren möglichen Bäumen wählt man den einfacheren\nEinfachheit gilt als Proxy für Generalisierungsfähigkeit\nPrinzip: Occam’s Razor – wähle die einfachste Hypothese, die zu den Daten passt",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#aufspaltung-eines-knotens",
    "href": "include/02_02_entscheidungsbaeume.html#aufspaltung-eines-knotens",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Aufspaltung eines Knotens",
    "text": "Aufspaltung eines Knotens\n\nsei \\(M = \\{(x_i, y_i)\\}_{i=1}^N\\) eine Trainingsmenge mit \\(K\\) Klassen\nsei \\(A\\) ein Attribut mit \\(v\\) Ausprägungen \\(A_1, \\dots, A_v\\)\naufteilung von \\(M\\) in Teilmengen \\(M_1, \\dots, M_v\\) gemäß Attribut \\(A\\)\n\\(M_j = \\{ (x_i, y_i) \\in M : x_i \\text{ hat Attributwert } A_j \\}\\)\nfür jeden Split-Knoten wird ein Attribut \\(A\\) gewählt, das \\(M\\) möglichst gut trennt also einen möglichst guten split erzeugt\ndie Qualität eines Splits wird über Entropie und Informationsgewinn gemessen\nZiel: maximale Homogenität der Klassen in den Teilmengen \\(M_j\\)\nBeispiel: wenn ein Attribut eine Partition erzeugt, in der jedes \\(M_j\\) nur eine Klasse enthält, ist der Informationsgewinn maximal\nfalls alle Klassen gleich verteilt bleiben, ist der Split nutzlos",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#entropie-als-maß-für-unreinheit",
    "href": "include/02_02_entscheidungsbaeume.html#entropie-als-maß-für-unreinheit",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Entropie als Maß für Unreinheit",
    "text": "Entropie als Maß für Unreinheit\n\nEntropie misst die Unreinheit (Unsicherheit) einer Klassenzuordnung in \\(M\\)\nje gemischter die Klassenverteilung, desto höher die Entropie\neine reine Menge (alle Objekte gleiche Klasse) hat Entropie \\(= 0\\)\nmaximal bei Gleichverteilung: \\(p_k = \\frac{1}{K}\\) für alle \\(k = 1, \\dots, K\\)\nEntropie entspricht dem erwarteten Informationsgehalt einer zufälligen Klassenzuordnung\nInterpretation: wie „überraschend“ ist das Ergebnis einer Klassifikation?\nbei deterministischer Klasse (z. B. 100 % sick): keine Überraschung \\(\\rightarrow\\) keine Information nötig\nbei Gleichverteilung (50/50): maximaler Überraschungseffekt \\(\\rightarrow\\) mehr Information nötig\nEntropie liefert Grundlage für den Informationsgewinn eines Attributsplits\nje stärker ein Attribut die Entropie reduziert, desto besser ist es zur Trennung geeignet",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#entropie-und-informationsgewinn",
    "href": "include/02_02_entscheidungsbaeume.html#entropie-und-informationsgewinn",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Entropie und Informationsgewinn",
    "text": "Entropie und Informationsgewinn\n\nsei \\(M\\) eine Menge von Objekten, die auf \\(K\\) Klassen verteilt sind\n\\(p_k\\) sei der Anteil der Objekte (rel. Häufigkeit) in Klasse \\(k\\), \\(k = 1, \\dots, K\\) und \\(m_1,\\ldots,m_K\\) die entsprechende Anzahl der Objekte\ndie Entropie von \\(M\\) ist definiert als: \\[\nH(M) = -\\sum_{k=1}^{K} p_k \\log_2 p_k\n\\]\n\\(H(M)\\) misst die Unreinheit (Unsicherheit) der Klassenverteilung in \\(M\\)\nwie zuvor: \\(A\\) ist Attribut mit \\(v\\) Ausprägungen: \\(a_1, \\dots, a_v\\) und \\(M\\) wird durch \\(A\\) in Teilmengen \\(M_1, \\dots, M_v\\) aufgeteilt; dann ist die gewichtete Entropie nach Aufteilung durch \\(A\\): \\[\nH_A(M) = \\sum_{j=1}^v \\frac{|M_j|}{|M|} \\cdot H(M_j)\\quad\\text{mit}\\quad\n\\frac{|M_j|}{|M|}=\\frac{\\sum_{k=1}^K m_{k}^{(j)}}{\\sum_{k=1}^K m_k}\\;,\n\\]\nwobei \\(m_{k}^{(j)}\\) die Anzahl der Objekte aus Klasse \\(k\\) innerhalb \\(M_j\\) bezeichnet\nder Informationsgewinn durch Attribut \\(A\\) ist definiert als: \\[\n\\text{gain}(A) = H(M) - H_A(M)\n\\]\n\\(A^* = \\arg\\max_{A \\in \\mathcal{A}} \\, \\text{gain}(A)\\): bestes Attribut für Aufspaltung maximiert gain über Menge aller verfügbaren Attribute \\(\\mathcal{A}\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-entropie-und-informationsgewinn",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-entropie-und-informationsgewinn",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Entropie und Informationsgewinn",
    "text": "Beispiel: Entropie und Informationsgewinn\n\ngegeben seien \\(K=2\\) Klassen: sick'' undhealthy’’ mit \\(p_1\\) als Anteil der Objekte in Klasse ``sick’’, \\(p_2 = 1 - p_1\\) für eine Menge von Objekten M\nInformationsgehalt eines Ereignisses \\(z\\) mit \\(p(z)\\) ist: \\[\nI_g(z) = -\\log_2 p(z)\n\\]\nIntuition: über ein wahrscheinliches Ereignis zu lernen ist weniger informativ als über ein unwahrscheinliches Ereignis zu lernen\nEntropie ist Erwartungswert des Informationsgehalts über alle Klassen und misst die Unbestimmtheit/Unreinheit der Klassenzugehörigkeit: \\[\nH(M) = p_1 \\cdot I_g(p_1) + p_2 \\cdot I_g(p_2)\n= -p_1 \\log_2 p_1 - p_2 \\log_2 p_2\n\\]\nExtremfall 1: \\(p_1 = 1\\), \\(p_2 = 0\\) \\(\\rightarrow\\) \\(H(M) = 0\\) (vollständige Sicherheit)\nExtremfall 2: \\(p_1 = p_2 = 0.5\\) \\(\\rightarrow\\) \\(H(M) = 1\\) (maximale Unsicherheit)\nInformationsgewinn durch ein Attribut \\(A\\): \\[\n\\text{gain}(A) = H(M) - H_A(M)\n\\]\nhoher Informationsgewinn von \\(A\\) erzeugt möglichst ``reinere’’ Teilmengen\nZiel: maximale Reduktion der Unreinheit durch Auswahl des besten \\(A^*\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-mit-entscheidungsbäumen",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-mit-entscheidungsbäumen",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Fußball mit Entscheidungsbäumen",
    "text": "Beispiel: Fußball mit Entscheidungsbäumen\n\n\n\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemperature\nHumidity\nWind\nPlay\n\n\n\n\nD1\nSun\nHot\nHigh\nLow\nNo\n\n\nD2\nSun\nHot\nHigh\nHigh\nNo\n\n\nD3\nOvercast\nHot\nHigh\nLow\nYes\n\n\nD4\nRain\nSweet\nHigh\nLow\nYes\n\n\nD5\nRain\nCool\nNormal\nLow\nYes\n\n\nD6\nRain\nCool\nNormal\nHigh\nNo\n\n\nD7\nOvercast\nCool\nNormal\nHigh\nYes\n\n\nD8\nSun\nSweet\nHigh\nLow\nNo\n\n\nD9\nSun\nCool\nNormal\nLow\nYes\n\n\nD10\nRain\nSweet\nNormal\nLow\nYes\n\n\nD11\nSun\nSweet\nNormal\nHigh\nYes\n\n\nD12\nOvercast\nSweet\nHigh\nHigh\nYes\n\n\nD13\nOvercast\nHot\nNormal\nLow\nYes\n\n\nD14\nRain\nSweet\nHigh\nHigh\nNo\n\n\n\nTitel: Trainingsdaten für die Klassifikation: Wetterbedingungen und Spielergebnis\n\nDaten: Wetterbedingungen an 14 Tagen, d.h. \\(M=14\\) zu klassifizierende Objekte, mit Attributen:\n\nOutlook: sun, overcast, rain\nTemperature: hot, sweet, cool\nHumidity: high, normal\nWind: high, low\n\nZielvariable der Klassifikation: ‘’Play’’\n\n\\(\\hookrightarrow\\) Ausprägungen Yes es wurde Fußball gespielt vs. No, es wurde nicht gespielt",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-mit-entscheidungsbäumen-1",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-mit-entscheidungsbäumen-1",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Fußball mit Entscheidungsbäumen",
    "text": "Beispiel: Fußball mit Entscheidungsbäumen\n\nZiel: finde Attribut mit maximalem Informationsgewinn als Wurzelknoten\n\n[1.] wir berechnen \\(H(M)\\)\n[2.] anschließend \\(H_{A_j}(M)\\) für alle vier Attribute \\(A_j\\), \\(j=1,\\ldots,4\\)\n[3.] Attribut mit maximalem \\(\\text{gain}(A_j)\\) bildet die Wurzel des Baums\n[4.] splitte den Baum auf Basis der Attributwerte in Unterbäume\n[5.] setze Schritte 2. - 4. fort, bis alle Elemente klassifiziert sind oder kein Informationsgewinn mehr möglich (Uneindeutigkeit des Klassifkationsproblems)\n\n\nKlassenverteilung: 9 \\(\\times\\) Yes, 5 \\(\\times\\) No: \\[\\begin{align}\nH(M) &= -\\frac{9}{14} \\log_2 \\frac{9}{14} - \\frac{5}{14} \\log_2 \\frac{5}{14}\n\\approx 0.940\\\\\nH(M_{\\text{sun}})  &= -\\frac{2}{5} \\log_2 \\frac{2}{5} - \\frac{3}{5} \\log_2\n\\frac{3}{5} \\approx 0.971 \\\\\nH(M_{\\text{overcast}}) & = -1 \\cdot \\log_2 1 = 0 \\\\\nH(M_{\\text{rain}}) &= -\\frac{3}{5} \\log_2 \\frac{3}{5} - \\frac{2}{5} \\log_2\n\\frac{2}{5} \\approx 0.971\\\\\n\\Rightarrow H_{\\text{Outlook}}(M) &= \\frac{5}{14} \\cdot 0.971 + \\frac{4}{14} \\cdot 0\n+ \\frac{5}{14} \\cdot 0.971 \\approx 0.693 \\\\\n\\Rightarrow \\text{gain}(\\text{Outlook}) &= H(M) - H_{\\text{Outlook}}(M)\n= 0.940 - 0.693 = 0.247\n\\end{align}\\]\nVergleich mit den anderen Informationsgewinnen:\ngain(Temperature)=0.029, gain(Humidity)=0.152, gain(Wind)=0.048\nFolge: Outlook wird als Wurzelknoten gewählt da gain(outlook)=0.247 am höchsten.",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-mit-entscheidungsbäumen-2",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-mit-entscheidungsbäumen-2",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Fußball mit Entscheidungsbäumen",
    "text": "Beispiel: Fußball mit Entscheidungsbäumen\n\n\nnach dem ersten Attributsplit entstehen wieder Teilbäume (s. Abb. 2.2)\nfür jeden dieser Teilbäume: erneut suchen des besten Attributs unter allen Attributen (einschließlich des ersten Outlook)\nFortsetzung bis alle Objekte eindeutig klassifiziert sind oder kein Informationsgewinn mehr möglich ist, z.B. wenn weitere Klassifizierung nicht mehr möglich ist: danach zufällige Festsetzung oder Mehrheitsklasse",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#c4.5-weiterentwicklung-von-id3",
    "href": "include/02_02_entscheidungsbaeume.html#c4.5-weiterentwicklung-von-id3",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "C4.5: Weiterentwicklung von ID3",
    "text": "C4.5: Weiterentwicklung von ID3\n\nC4.5 verwendet wie ID3 Entropie als Kriterium, verbessert jedoch die Auswahl der Attribute\nProblem: Informationsgewinn bevorzugt Attribute mit vielen Ausprägungen\nLösung: gainRatio als Quotient aus Informationsgewinn und Entropie der Aufteilung ‘’Split-Entropie’’: \\[\n\\text{gainRatio}(A) = \\frac{\\text{gain}(A)}{\\text{SplitEntropy}(A)}\n\\]\nSplit Entropy: misst die Informationsmenge zur Beschreibung der Aufteilung selbst: \\[\n\\text{SplitEntropy}(A) = -\\sum_{j=1}^v \\frac{|M_j|}{|M|}\n\\log_2 \\frac{|M_j|}{|M|}\n\\]\nje mehr mögliche Ausprägungen ein Attribut hat, desto höher die Split-Entropie\nInformationsgewinn allein belohnt ‘’überpräzise’’ Splits, z.B. durch ein Attribut mit fast eindeutigem Wert\ngainRatio normalisiert diesen Effekt und bevorzugt trennscharfe und gleichzeitig generalisierende Attribute",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#cart-grundidee",
    "href": "include/02_02_entscheidungsbaeume.html#cart-grundidee",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "CART: Grundidee",
    "text": "CART: Grundidee\n\nCART beginnt mit einem einzelnen Knoten \\(L_0\\) (der Wurzel) und Menge von Objekten \\(M_0\\) (\\(=M\\))\njeder Knoten \\(L_q\\) verarbeitet eine Teilmenge \\(M_q \\subseteq M\\) der Daten\nwenn alle Elemente in \\(M_q\\) die gleiche Klasse haben \\(\\rightarrow\\) \\(L_q\\) wird Blatt\nfalls für eine Untermenge \\(M_q\\) kein Informationsgewinn mehr möglich ist:\n\ndann zufällige Festsetzung der Kategoriezugehörigkeit\noder Festsetzung der Kategoriezugehörigkeit mit Wert der Mehrheitskategorie\noder als Mittelwert über Kategorien (bei metrischen Werten z.B. Regressionsbäumen)\n\nsonst: wähle Attribut \\(A_i\\) unter allen Attributen, für \\(i=1,\\ldots,v\\), mit Menge von Attributwerten \\(\\mathcal{A}_i\\), sodass eine optimale Aufspaltung von \\(M_q\\) entsteht\n\n\\(\\hookrightarrow\\) Optimalitätsmaß: nicht Entropie sondern Gini-Index !\n\nes werden binäre Partitionen \\(\\mathcal{A}_i^L \\cup \\mathcal{A}_i^R = \\mathcal{A}_i\\), ~~ \\(\\mathcal{A}_i^L \\cap \\mathcal{A}_i^R = \\emptyset\\) betrachtet; diese definieren dann auch eine Zerlegung von \\(M_q\\) zwei ebenfalls disjunkte Teilmengen: \\[\\begin{align}\nL_q^i(\\mathcal{A}_i^L)&=\\lbrace x^{(m)}\\in M_q:a_i^{m}\\in \\mathcal{A}_i^L\\rbrace\\\\\nR_q^i(\\mathcal{A}_i^R)&=\\lbrace x^{(m)}\\in M_q:a_i^{m}\\in \\mathcal{A}_i^R\\rbrace\n\\end{align}\\]\nwobei \\(a_i^{m}\\) die konkrete Ausprägung eines Attributs \\(A_i\\) für ein zu klassifizierendes Input(-objekt) \\(x^{(m)}\\) bezeichnet und die Wahrscheinlichkeiten bzw. Anteile: \\(p_{iL}^q(\\mathcal{A}_i^L) = \\frac{L_q^i(\\mathcal{A}_i^L)}{M_q}\\) und \\(p_{iR}^q(\\mathcal{A}_i^R) = 1 - p_{iL}^q(\\mathcal{A}_i^L)\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#cart-gini-index-und-knotenaufteilung",
    "href": "include/02_02_entscheidungsbaeume.html#cart-gini-index-und-knotenaufteilung",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "CART: Gini-Index und Knotenaufteilung",
    "text": "CART: Gini-Index und Knotenaufteilung\n\nGini-Index misst Unreinheit in \\(M_q\\) mit Klassenanteilen \\(p_k^q\\): \\[\nGini(M_q) = 1 - \\sum_{k=1}^K (p_k^q)^2\n\\]\nminimale Unreinheit bei homogener Klassenzuordnung: \\(Gini(M_q) = 0\\)\nmaximale Unreinheit bei Gleichverteilung: \\(p_k^q = \\frac{1}{K}\\)\nAufteilung von \\(M_q\\) in zwei Mengen über Zerlegung \\(\\mathcal{A}_i^L, \\mathcal{A}_i^R\\) und Berechnung eines gewichteten Gini-Index nach Aufteilung: \\[\\begin{align}\nwGini(M_q, \\mathcal{A}_i^L) &= p_{iL}^q \\cdot\nGini(L_q^i(\\mathcal{A}_i^L)) + p_{iR}^q \\cdot\nGini(R_q^i(\\mathcal{A}_i^R))\\\\\nGini(L_q^i(\\mathcal{A}_i^L)) &= 1 - \\sum_{k=1}^K\n(p_{kL}^q(\\mathcal{A}_i^L))^2\\\\\nGini(R_q^i(\\mathcal{A}_i^R)) &= 1 - \\sum_{k=1}^K\n(p_{kR}^q(\\mathcal{A}_i^R))^2\n\\end{align}\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#cart-gini-gewinn-und-optimierung",
    "href": "include/02_02_entscheidungsbaeume.html#cart-gini-gewinn-und-optimierung",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "CART: Gini-Gewinn und Optimierung",
    "text": "CART: Gini-Gewinn und Optimierung\n\nGini-Gewinn durch Partition \\(\\mathcal{A}_i^L \\subset \\mathcal{A}_i\\): \\[\nGiniGain(M_q, \\mathcal{A}_i^L) = Gini(M_q) - wGini(M_q, \\mathcal{A}_i^L)\n\\]\nZiel: finde Partition mit maximalem Gini-Gewinn \\[\n\\mathcal{A}_i^{L,*} = \\arg\\max_{\\mathcal{A}_i^L \\subset \\mathcal{A}_i} \\,\nGiniGain(M_q, \\mathcal{A}_i^L)\n\\]\nfalls Gini-Gewinn = 0 \\(\\rightarrow\\) keine sinnvolle Aufspaltung möglich und Knoten \\(L_q\\) erzeugt dann binäre Kinderknoten, basierend auf der gefundenen optimalen Partition induziert durch \\(\\mathcal{A}_i^{L,*}\\)\ndie binären Kinderknoten erhalten dann jeweils ihre zugehörigen Teilmengen \\(M_L = L_q(\\mathcal{A}_i^{L,*})\\), \\(M_R = R_q(\\mathcal{A}_i^{L,*})\\) und die Aufspaltung beginnt von neuem",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#cart-eigenschaften-und-pruning",
    "href": "include/02_02_entscheidungsbaeume.html#cart-eigenschaften-und-pruning",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "CART: Eigenschaften und Pruning",
    "text": "CART: Eigenschaften und Pruning\n\nCART verwendet ausschließlich binäre Aufteilungen\nanwendbar auf numerische und kategoriale Attribute\n\nbei numerischen Werten basiert Aufteilung auf einem threshold Wert der Form: \\(x^{(m)} \\leq t \\quad \\text{vs.} \\quad x^{(m)} &gt; t\\)\nkategoriale Merkmale werden in zwei Gruppen aufgeteilt z. B. \\(\\{\\text{sun, rain}\\}\\) vs. \\(\\{\\text{overcast}\\}\\)\n\ndabei werden alle möglichen Kombinationen geprüft:\n\n\\(\\{\\text{sun, rain}\\}\\) vs. \\(\\{\\text{overcast}\\}\\)\n\\(\\{\\text{sun}\\}\\) vs. \\(\\{\\text{overcast, rain}\\}\\)\n…\n\n\n\nerzeugte Bäume sind oft robuster als bei ID3\nzur Vermeidung von Overfitting wird Pruning eingesetzt:\n\nPre-Pruning: Stoppen der Aufspaltung bei geringer Stichprobengröße oder kleinem Gini-Gewinn\nPost-Pruning: nachträgliches Zurückschneiden unnötiger Teilbäume\n\nPost-Pruning liefert in der Regel bessere Resultate",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#regression-mit-entscheidungsbäumen",
    "href": "include/02_02_entscheidungsbaeume.html#regression-mit-entscheidungsbäumen",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Regression mit Entscheidungsbäumen",
    "text": "Regression mit Entscheidungsbäumen\n\nZiel: Vorhersage einer numerischen Zielvariable, z. B. Anzahl Spieler\nStruktur des Baums wie bei CART: binäre Splits entlang von Attributen\njeder Blattknoten enthält den arithmetischen Mittelwert der Zielvariable\nkein Klassenlabel, sondern reelle Vorhersagegröße\nKriterium für Attributwahl: Varianzreduktion\nfür Teilmenge \\(M_q\\): \\[\n\\text{Var}(M_q) = \\frac{1}{|M_q|} \\sum_{i \\in M_q} (y_i - \\bar{y}_q)^2\n\\]\nAufteilung in \\(M_1\\), \\(M_2\\) mit Gewichtung: \\[\n\\text{Var}_{\\text{split}} = \\frac{|M_1|}{|M_q|} \\text{Var}(M_1) +\n\\frac{|M_2|}{|M_q|} \\text{Var}(M_2)\n\\]\nZiel: maximale Reduktion der Varianz \\[\n\\text{gain}_\\text{Var} = \\text{Var}(M_q) - \\text{Var}_{\\text{split}}\n\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-datensatz-regression",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-datensatz-regression",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Fußball-Datensatz (Regression)",
    "text": "Beispiel: Fußball-Datensatz (Regression)\n\n\n\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemperature\nHumidity\nWind\nPlayers\n\n\n\n\nD1\nSun\nHot\nHigh\nLow\n25\n\n\nD2\nSun\nHot\nHigh\nHigh\n30\n\n\nD3\nOvercast\nHot\nHigh\nLow\n46\n\n\nD4\nRain\nSweet\nHigh\nLow\n45\n\n\nD5\nRain\nCool\nNormal\nLow\n52\n\n\nD6\nRain\nCool\nNormal\nHigh\n23\n\n\nD7\nOvercast\nCool\nNormal\nHigh\n43\n\n\nD8\nSun\nSweet\nHigh\nLow\n35\n\n\nD9\nSun\nCool\nNormal\nLow\n38\n\n\nD10\nRain\nSweet\nNormal\nLow\n46\n\n\nD11\nSun\nSweet\nNormal\nHigh\n48\n\n\nD12\nOvercast\nSweet\nHigh\nHigh\n52\n\n\nD13\nOvercast\nHot\nNormal\nLow\n44\n\n\nD14\nRain\nSweet\nHigh\nHigh\n30\n\n\n\nTitel: Trainingsdaten für die Regression: Wetterbedingungen und Spieleranzahl\n\nZiel: Vorhersage der Spieleranzahl (numerisch) \\(\\rightarrow\\) Beispielattribut ``Outlook’’ mit drei Ausprägungen\n\n\n\n\n\n\n\n\n\n\n\nOutlook\nAnzahl\nMittelwert\nVarianz\nStd.-Abw.\n\n\n\n\nOvercast\n4\n46.25\n14.19\n3.77\n\n\nRain\n5\n39.20\n101.36\n10.07\n\n\nSun\n5\n35.20\n86.56\n9.30\n\n\n\nTitel: Spieleranzahl nach Outlook: Mittelwert, Varianz und Standardabweichung\nFormel Varianz und Standardabweichung: \\[\n\\text{Var}(M) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2\\;,\\quad\n\\text{SD}(M) = \\sqrt{\\text{Var}(M)}\n\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-datensatz-regression-1",
    "href": "include/02_02_entscheidungsbaeume.html#beispiel-fußball-datensatz-regression-1",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Beispiel: Fußball-Datensatz (Regression)",
    "text": "Beispiel: Fußball-Datensatz (Regression)\n\nGesamtvarianz (korrekt): \\(86.88\\) Gesamt-Std.-Abw. (wie im SB): \\(9.32\\)\ngewichtete Varianz nach Outlook: \\[\\begin{align}\n\\bar{y} &= \\frac{1}{14} \\sum_{i=1}^{14} y_i\n= \\frac{607}{14} \\approx 43.36 \\\\[1ex]\n\\text{Var}(M) &= \\frac{1}{14} \\sum_{i=1}^{14} (y_i - \\bar{y})^2\n= \\frac{1}{14} \\left[\n(25 - 43.36)^2 + (30 - 43.36)^2 + \\dots + (30 - 43.36)^2 \\right] \\\\\n&= \\frac{1}{14} \\cdot 1216.26 \\approx 86.88 \\\\[1ex]\n\\text{SD}(M) &= \\sqrt{86.88} \\approx 9.32\n\\end{align}\\]\nVarianzreduktion: $ _ = 86.88 - 67.31 = 19.57 $\ngewichtete SD nach Outlook: \\[\n\\text{SD}(M) = 9.32 \\\\[1ex]\n\\text{SD}_{\\text{split}} =\n\\frac{5}{14} \\cdot 9.30 + \\frac{4}{14} \\cdot 3.77 +\n\\frac{5}{14} \\cdot 10.07\n= 3.32 + 1.08 + 3.26 = 7.66 \\\\[1ex]\n\\]\n``gain’’ auf Basis SD: \\(\\text{gain}_\\text{SD} = 9.32 - 7.66 = 1.66\\)\nSplitkriterium: Wähle Attribut mit maximaler Varianzreduktion, oder basierend auf Standardabweichung\nBlattwert: arithmetisches Mittel der Zielwerte in jedem Blatt",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "include/02_02_entscheidungsbaeume.html#random-forest",
    "href": "include/02_02_entscheidungsbaeume.html#random-forest",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Random Forest",
    "text": "Random Forest\n\nEnsemble von Entscheidungsbäumen\n\nRandom Forest kombiniert viele Entscheidungsbäume zu einem Modell\nIdee: Bootstrapping + zufällige Attributauswahl pro Split\njeder Baum wird auf einer Zufallsstichprobe der Daten trainiert\nbei jedem Split wird eine Zufallsauswahl von Attributen betrachtet\nVorhersage bei Regression vs. Klassifikation: \\[\n\\hat{y}_i = \\frac{1}{T} \\sum_{t=1}^T \\hat{y}_{it}\\;, \\quad\n\\hat{p}_i = \\frac{1}{T} \\sum_{t=1}^T I(\\hat{y}_{it} = 1)\n\\quad \\Rightarrow \\quad\n\\hat{y}_i =\n\\begin{cases}\n1, & \\hat{p}_i &gt; 0.5 \\\\\n0, & \\text{sonst}\n\\end{cases}\\;.\n\\]\n\n\n\nRandom Forest: Eigenschaften\n\nreduziert die Varianz einzelner Bäume durch Aggregation\nrobust gegenüber Rauschen und Overfitting\nkeine Pruning-Strategien notwendig\nkeine starke Parametrierung erforderlich (nur Anzahl Bäume \\(T\\) und Attributanzahl \\(m\\) pro Split)\nEnsemble-Vorhersagen sind oft genauer und stabiler als Einzelbäume\nNachteile: geringe Interpretierbarkeit, hoher Rechenaufwand bei vielen Bäumen",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Entscheidungsbäume"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Welcome! Hier findest du Skripte, Beispiele und Python‑Code (Notebooks) für den Kurs. Starte mit einem der Bereiche oder nutze die Suche links.\n\n Start: Studienbrief 01   Start: Studienbrief 02   Entscheidungsbäume   Neuronale Netze I: SLP   Neuronale Netze II: MLP   Altklausuren \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWICHTIG: Alle relevanten und verbindlichen Informationen (Klausurtermine / Änderungen / Regularien) nur auf den HFH Seiten und im Webcampus \\(\\ra\\) regelmäßig vorbeischauen !\n\n\n\n\n\n\n\n\nCaution\n\n\n\nHinweise: Klausurtermin 01: 13.09.2025\n\n\n\n\n\n\n\n\nCaution\n\n\n\nHinweise: Klausurtermin 02: 13.12.2025\n\n\n\n\n\n\n\n\nTip\n\n\n\nHinweise: Folien und Notebooks werden laufend ergänzt. Bei Fragen oder Fehlern bitte im GitHub-Menü (links oben) melden.\n\n\n\n\n\nIlya Zarubin (M.Sc.)\nE‑mail: ilya.zarub@campus.hamburger-fh.de\n\nKontakt Über",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#ablauf-und-ziel",
    "href": "index.html#ablauf-und-ziel",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Ablauf und Ziel",
    "text": "Ablauf und Ziel\n\nKurzer Überblick über SB 01 – Fokus: potentielle Klausuraufgaben\nRest von SB 01 lässt sich eigenständig gut lesen und verstehen \\(\\ra\\) Zeit sparen !\nSB 02: Fokus auf Theorie von Entscheidungsbäumen und neuronalen Netzen\nSB 02: Klausuraufgaben bzgl. ID3 und Cart Bäume\nSB 03 und Beginn SB 04: Implementierung von Konzepten aus SB 02 in Python\nSB 03: Klausuraufgaben Programmierung\nSB 04: evolutionäre Algorithmen: Theorie und Anwedung/Programmierung zusammen\nSB 04: Klausuraufgaben\nSB 05: weitere evolutionäre Algorithmen: Theorie und Anwedung/Programmierung zusammen – Ameisen und particle swarm\nSB 05: Klausuraufgaben",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "reale Daten sind oft verrauscht, unvollständig oder inkonsistent\nZiel: Datenqualität verbessern, damit ML-Algorithmen effektiv lernen\ntypische Probleme: fehlende Werte, Ausreißer, falsches Format, Rauschen\nMerkmale (Features) = Eingabegrößen für das Modell",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#verarbeitung-und-erzeugen-von-daten",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#verarbeitung-und-erzeugen-von-daten",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "reale Daten sind oft verrauscht, unvollständig oder inkonsistent\nZiel: Datenqualität verbessern, damit ML-Algorithmen effektiv lernen\ntypische Probleme: fehlende Werte, Ausreißer, falsches Format, Rauschen\nMerkmale (Features) = Eingabegrößen für das Modell",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#datenbereinigung",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#datenbereinigung",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Datenbereinigung",
    "text": "Datenbereinigung\n\nZiel: Korrektur oder Entfernung fehlerhafter Einträge\nMethoden:\n\nfehlende Werte ignorieren oder durch Mittelwert, Regression o. Ä. ersetzen\nRauschentfernung durch Binning, Regression oder Clustering\nAusreißerbehandlung mittels Clustering oder Entfernung\n\n\n\nKlausurrelevant: schonmal abgefragt (s. Altklausuren)",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#integration-transformation-und-reduktion",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#integration-transformation-und-reduktion",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Integration, Transformation und Reduktion",
    "text": "Integration, Transformation und Reduktion\n\nDatenintegration\n\nZusammenführung verschiedener Datenquellen in eine einheitliche Struktur\ntypische Probleme:\n\nSchemaintegration (uneinheitliche Formate)\nredundante Attribute\nDatenwertkonflikte (z. B. unterschiedliche Einheiten)\n\n\n\n\nDatentransformation\n\nZiel: Daten vereinheitlichen, um Verarbeitbarkeit zu verbessern\nTechniken: Normalisierung, Aggregation, Generalisierung, Attributbildung\n\n\n\nKlausurrelevant: schonmal abgefragt (s. Altklausuren)",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#datenreduktion",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#datenreduktion",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Datenreduktion",
    "text": "Datenreduktion\n\nReduktion des Datenvolumens bei Erhalt der Analysequalität\nVerfahren: Datenkompression, Dimensionsreduktion, Diskretisierung\nZiel: schnellere Verarbeitung bei geringem Informationsverlust\n\n\nKlausurrelevant: schonmal abgefragt (s. Altklausuren)",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#datenerweiterung-klassische-augmentation",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#datenerweiterung-klassische-augmentation",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Datenerweiterung: klassische Augmentation",
    "text": "Datenerweiterung: klassische Augmentation\n\nMotivation und Prinzip\n\nZiel: künstliche Vergrößerung von Trainingsdatensätzen\nhilfreich bei kleinen Datenbasen oder zur Erhöhung der Robustheit\nneue Datenpunkte entstehen durch gezielte Veränderungen bestehender Beispiele\nbesonders verbreitet bei visuellen Daten\n\n\n\ntypische Bildtransformationen\n\nDrehen, Spiegeln, Zoomen, Zuschneiden, Verschieben (Translation)\nFarbänderung, Helligkeit/Kontrast anpassen, Rauschen hinzufügen\nGraustufenumwandlung, zufälliges Löschen (Cutout)",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#datenerzeugung-mit-deep-learning",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#datenerzeugung-mit-deep-learning",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Datenerzeugung mit Deep Learning",
    "text": "Datenerzeugung mit Deep Learning\n\nGANs und neuronale Stilübertragung\n\nGenerative Adversarial Networks (GAN):\n\nbestehen aus Generator und Diskriminator im Wettstreit\nerzeugen neue Datenpunkte, die echten Daten ähneln\n\nNeuronale Stilübertragung:\n\nkombiniert Inhaltsbild und Stilbild zu synthetischem Output\nOutput enthält die Inhalte des ersten Bilds im Stil des zweiten",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#datenerweiterung-vor--und-nachteile",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#datenerweiterung-vor--und-nachteile",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Datenerweiterung: Vor- und Nachteile",
    "text": "Datenerweiterung: Vor- und Nachteile\n\nDatenerweiterung kann Genauigkeit und Robustheit von KI verbessern\nsie ersetzt jedoch keine hochwertigen Originaldaten\nRisiken: Reproduktion von Verzerrungen, mangelhafte Qualität künstlicher Daten\nbesonders kritisch bei ethischen oder sensiblen Anwendungen\n\n\nPotentiell Klausurrelevant: aber noch nie abgefragt",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_03_verarbeitung_erzeugung_daten.html#simulation-als-künstliche-datenquelle",
    "href": "include/01_03_verarbeitung_erzeugung_daten.html#simulation-als-künstliche-datenquelle",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Simulation als künstliche Datenquelle",
    "text": "Simulation als künstliche Datenquelle\n\nPrinzip und Zielsetzung\n\nSimulation = virtuelle Nachbildung realer Prozesse auf Basis mathematischer Modelle\nZiel: Erzeugung realistischer, aber kontrollierter Daten ohne physische Durchführung\nzentrale Bestandteile:\n\nModell des Systems (z. B. physikalisch, ökonomisch, biologisch)\nEingabeparameter, die Systemverhalten definieren\nAusgabevariablen, die analysiert und als Trainingsdaten genutzt werden\n\n\n\n\nVorteile gegenüber realer Datenerhebung\n\nkeine Risiken für Personen, Geräte oder Infrastruktur\nflexibel skalierbar, auch für seltene/extreme Szenarien\nvollständige Kontrolle über Ground Truth und Umweltvariablen\ngeeignet bei Datenschutzproblemen oder nicht zugänglichen Szenarien\n\n\n\nAnwendungsfelder\n\nReinforcement Learning (RL):\n\nTraining von Agenten via Trial & Error in sicherer Simulationsumgebung\nspätere Übertragung des Verhaltens in reale Systeme (z. B. Roboter)\n\nForecasting (z. B. Energie, Logistik), Anomaliedetektion, Risikobewertung\nSimulation synthetischer Bilddaten (z. B. für medizinische Diagnostik)",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Verarbeitung und Erzeugen von Daten"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Computer Vision (CV) ist ein Teilgebiet der KI zur automatisierten Auswertung visueller Informationen wie Bilder oder Videos\nZiel: Interpretation visueller Daten auf einem Niveau, das menschlichem Sehen vergleichbar ist – für Klassifikation, Analyse, Steuerung\nzentrale Aufgabenbereiche:\n\nBildklassifikation: z. B. Diagnoseverfahren, Qualitätssicherung\nObjekterkennung: z. B. autonome Systeme, Sicherheitstechnik\nSegmentierung und Rekonstruktion: z. B. in Medizin und Robotik\n\nwichtige Unterscheidungen:\n\nBildverarbeitung: technische Modifikation von Bilddaten\nBilderkennung: interpretative Einordnung – entscheidungsrelevant\n\nDeep Learning ermöglicht z. B. automatische Klassifikation & visuelle Synthese",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-computer-vision",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-computer-vision",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Computer Vision (CV) ist ein Teilgebiet der KI zur automatisierten Auswertung visueller Informationen wie Bilder oder Videos\nZiel: Interpretation visueller Daten auf einem Niveau, das menschlichem Sehen vergleichbar ist – für Klassifikation, Analyse, Steuerung\nzentrale Aufgabenbereiche:\n\nBildklassifikation: z. B. Diagnoseverfahren, Qualitätssicherung\nObjekterkennung: z. B. autonome Systeme, Sicherheitstechnik\nSegmentierung und Rekonstruktion: z. B. in Medizin und Robotik\n\nwichtige Unterscheidungen:\n\nBildverarbeitung: technische Modifikation von Bilddaten\nBilderkennung: interpretative Einordnung – entscheidungsrelevant\n\nDeep Learning ermöglicht z. B. automatische Klassifikation & visuelle Synthese",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-computer-vision-1",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-computer-vision-1",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendung: Computer Vision",
    "text": "Anwendung: Computer Vision\n\nAndwendungsfälle aus Bildverarbeitung/-erkennung",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-natural-language-processing-nlp",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-natural-language-processing-nlp",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendung: Natural Language Processing (NLP)",
    "text": "Anwendung: Natural Language Processing (NLP)\n\nGrundlagen und Anwendungsfelder\n\nNLP ermöglicht Maschinen das Verstehen, Interpretieren und Generieren menschlicher Sprache (Text & Audio)\ntypische Aufgaben:\n\nmaschinelle Übersetzung, Autokorrektur, Autovervollständigung\nStimmungsanalyse, Spracherkennung, Chatbots\n\nSprachverarbeitung = Kombination aus Linguistik, Statistik und KI\nmoderne NLP-Systeme basieren auf Deep Learning, v. a. Transformer-Architekturen\nSprachmodelle (z. B. GPT, LaMDA, Megatron) ermöglichen kontextsensitives Verstehen und Generieren ganzer Absätze",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-natural-language-processing-nlp-1",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-natural-language-processing-nlp-1",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendung: Natural Language Processing (NLP)",
    "text": "Anwendung: Natural Language Processing (NLP)\n\nAnwendungsfelder",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-natural-language-processing-nlp-2",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-natural-language-processing-nlp-2",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendung: Natural Language Processing (NLP)",
    "text": "Anwendung: Natural Language Processing (NLP)\n\nAnwendungsfelder",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#nlp-entwicklung-großer-sprachmodelle",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#nlp-entwicklung-großer-sprachmodelle",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "NLP: Entwicklung großer Sprachmodelle",
    "text": "NLP: Entwicklung großer Sprachmodelle\n\nEntwicklung Parameter Zahl der Modelle",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#nlp-entwicklung-großer-sprachmodelle-1",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#nlp-entwicklung-großer-sprachmodelle-1",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "NLP: Entwicklung großer Sprachmodelle",
    "text": "NLP: Entwicklung großer Sprachmodelle\n\nNeueste Entwicklungen (bis 2024)\n\nmoderne Sprachmodelle basieren auf Transformer-Architekturen\nstetiger Anstieg der Parameteranzahl seit 2018 (GPT-2: 0,1 Mrd. \\(\\ra\\) GPT-4: $$1 Bio. geschätzt)\nwichtige Meilensteine:\n\nGPT-3 (2020): 175 Mrd. Parameter\nGPT-4 (2023): multimodal, deutlich robuster, genaue Architektur nicht veröffentlicht\nGPT-4 Turbo (2023): kosteneffizientere Variante mit gleichem Verhalten\nClaude 2 (Anthropic), Gemini (Google), LLaMA 2 (Meta) als offene Alternativen\n\nTrends: multimodale Fähigkeiten, Tool-Integration (z. B. Code, Bilder, Audio), RLHF\nHerausforderungen: Bias, Halluzination, Energiebedarf, Erklärbarkeit",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-biologie-alphafold",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-biologie-alphafold",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendung: Biologie – AlphaFold",
    "text": "Anwendung: Biologie – AlphaFold\n\nProteinfaltung und Strukturvorhersage\n\nProteine: Ketten aus Aminosäuren, deren 3D-Struktur ihre Funktion bestimmt\nProblem: experimentelle Strukturbestimmung ist teuer & langsam\nZiel: Strukturvorhersage allein aus Sequenzinformationen\nAlphaFold (DeepMind):\n\nverwendet neuronale Netze mit Attention & evolutionären Features\nVorhersage von Abständen & Winkeln zwischen Aminosäuren\ntrainiert auf 100.000+ bekannte Proteinstrukturen (PDB)\n\nValidierung durch CASP14 (2020):\n\nGDT (Global Distance Test): Metrik zur Strukturähnlichkeit (0–100)\nAlphaFold erreichte GDT-Werte $$90 – nahe experimenteller Genauigkeit",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#alphafold-ergebnisse-im-casp14-wettbewerb",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#alphafold-ergebnisse-im-casp14-wettbewerb",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "AlphaFold: Ergebnisse im CASP14-Wettbewerb",
    "text": "AlphaFold: Ergebnisse im CASP14-Wettbewerb",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-autonomes-fahren",
    "href": "include/01_04_anwendungsmoeglichkeit_ki.html#anwendung-autonomes-fahren",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendung: Autonomes Fahren",
    "text": "Anwendung: Autonomes Fahren\n\nKI in selbstfahrenden Fahrzeugen\n\nautonome Fahrzeuge (AVs) nutzen KI zur:\n\nWahrnehmung der Umgebung (Kameras, Lidar, Radar)\nLokalisierung & Kartierung (SLAM, GPS)\nEntscheidungsfindung (Routenplanung, Verhalten)\nKontrolle (Lenkung, Beschleunigung, Bremsen)\n\nzentrale Rolle von Deep Learning bei:\n\nObjekterkennung, Segmentierung, Fußgängerprognose\nSzenenverständnis, Vorausschau, Gefahrenerkennung\n\nVorreiter: Waymo, hervorgegangen aus dem Google-Auto-Projekt\n\nnutzt DNNs für Fußgängererkennung, Planung & Simulation\n\n10 Mio reale Kilometer + &gt;10 Mrd Simulationskilometer\n\n\nHerausforderungen:\n\nkomplexe urbane Szenarien, rare edge cases, ethische Entscheidungen\nRechenleistung (Edge), Latenz, Zertifizierung & Sicherheit",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Anwendungsmöglichkeiten und Beispiele für Methoden der künstlichen Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "klassische Software muss bei Prozessänderungen neu angepasst werden\ndies verursacht hohe Kosten und Instandhaltungsaufwand\nkünstliche Intelligenz (KI) soll sich adaptiv anpassen können\nZiel: Systeme, die selbst lernen und Wissen übertragen können\nKI nutzt große Datenmengen zur Lösung komplexer Aufgaben\n\n\n\n\n\nKI vereint Erkenntnisse aus Logik, Statistik, Neurobiologie, Linguistik usw.\nZiel: Systeme, die intelligent auf Probleme reagieren können\nkeine einheitliche Definition, aber zentrale Eigenschaften:\n\nselbständige Problembearbeitung\nlernfähig, adaptiv, generalisierend",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#zielsetzung-und-definition",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#zielsetzung-und-definition",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "klassische Software muss bei Prozessänderungen neu angepasst werden\ndies verursacht hohe Kosten und Instandhaltungsaufwand\nkünstliche Intelligenz (KI) soll sich adaptiv anpassen können\nZiel: Systeme, die selbst lernen und Wissen übertragen können\nKI nutzt große Datenmengen zur Lösung komplexer Aufgaben\n\n\n\n\n\nKI vereint Erkenntnisse aus Logik, Statistik, Neurobiologie, Linguistik usw.\nZiel: Systeme, die intelligent auf Probleme reagieren können\nkeine einheitliche Definition, aber zentrale Eigenschaften:\n\nselbständige Problembearbeitung\nlernfähig, adaptiv, generalisierend",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#starke-vs.-schwache-ki-anwendungsfelder",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#starke-vs.-schwache-ki-anwendungsfelder",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "starke vs. schwache KI – Anwendungsfelder",
    "text": "starke vs. schwache KI – Anwendungsfelder\n\nstarke vs. schwache KI\n\nstarke KI: simuliert menschliches Denken & Bewusstsein\nZiel: vollständiger Ersatz menschlicher Intelligenz in allen Bereichen\nbisher rein hypothetisch, keine realen Anwendungen\nschwache KI: löst spezifische Aufgaben auf intelligente Weise\nbasiert auf Algorithmen, heute weit verbreitet\n\n\n\nHistorische Ursprünge & Begriffsprägung\n\nBegriff „Artificial Intelligence“ durch John McCarthy (1956)\nZiel: Maschinen bauen, die intelligentes Verhalten imitieren\nAlan Turing: Maschine ist intelligent, wenn sie nicht erkennbar ist\nÜbersetzung „künstlich“ statt „simuliert“ prägt deutsches Verständnis",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#anwendungsfelder-von-ki",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#anwendungsfelder-von-ki",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Anwendungsfelder von KI",
    "text": "Anwendungsfelder von KI\n\nSprachverarbeitung (NLP)\nBildverarbeitung (NIP)\nExpertensysteme mit breiter Wissensbasis\nRobotik: autonome mechanische Systeme\nKombination dieser Felder in der Praxis üblich\n\n\n\n\n\n\n\nFigure 1: Abbildung",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#geschichte-der-ki-entwicklungsetappen",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#geschichte-der-ki-entwicklungsetappen",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Geschichte der KI & Entwicklungsetappen",
    "text": "Geschichte der KI & Entwicklungsetappen\n\nGeschichte der KI\n\nBegriff „Artificial Intelligence“ stammt von John McCarthy (1956)\nZiel: Maschinen bauen, die Aspekte menschlicher Intelligenz nachahmen\nBegriff wurde später kritisch gesehen, setzte sich aber durch\nAlan Turing prägte mit dem Turing-Test eine erste KI-Definition\nWenn ein Mensch nicht erkennt, ob er mit Maschine oder Mensch spricht: Maschine gilt als intelligent\n\n\n\nEntwicklungsetappen\n\n1966: ELIZA – regelbasierter Chatbot von Weizenbaum\n\nReaktion: naives Vertrauen in Maschinen\nWeizenbaum wurde später KI-Kritiker\n\n1973: PROLOG – logikorientierte Sprache, stark in Europa\n1980er: KI-Hype – kommerzielles PROLOG, erste Firmenlösungen\n1988: DFKI-Gründung – deutsches KI-Zentrum mit &gt;250 Projekten\n1997: RoboCup – Fußballturnier autonomer Roboter\n2000er: ML und Bayes – maschinelles Lernen gewinnt an Relevanz",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#meilensteine-der-ki",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#meilensteine-der-ki",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Meilensteine der KI",
    "text": "Meilensteine der KI\n\n1637: René Descartes – unterscheidet erstmals zwischen ``automatischer Maschine’’ und lernfähigem System\n1950: Alan Turing – entwickelt den Turing-Test als Kriterium für maschinelle Intelligenz\n1956: John McCarthy – organisiert die KI-Gründungskonferenz am Dartmouth College, USA\n1966: ELIZA (Joseph Weizenbaum) – erster Chatbot, erzeugt große öffentliche Resonanz\n1982: Japan startet Fünfte-Generation-Projekt – 400 Mio. Dollar für KI-Entwicklung, bleibt ohne große Erfolge\n1996: Deep Blue (IBM) – besiegt Schachweltmeister Garri Kasparow\n2002: ROOMBA (iRobot) – erster autonomer Haushaltsroboter im Massenmarkt\n2010: Watson (IBM) – gewinnt gegen menschliche Champions in der Quizshow JEOPARDY!\n2011: Siri (Apple) – KI-basierter Sprachassistent wird massentauglich und Teil von iOS\n2015: Bayesian Program Learning (MIT, NYU, Toronto) – KI erkennt handgeschriebene Zeichen besser als Menschen, oft nach nur einem Beispiel\n2016: AlphaGo (DeepMind) – besiegt Go-Weltmeister Lee Sedol mit kreativen Zügen, die zuvor als ``nicht menschlich’’ galten\n2017: Waymo (Google-Tochter) – startet Testbetrieb autonomer Taxis in Phoenix, Arizona",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#rechtlicher-rahmen",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#rechtlicher-rahmen",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Rechtlicher Rahmen",
    "text": "Rechtlicher Rahmen\n\nChancen und Risiken\n\nKI bietet enormes Potenzial – aber auch Risiken für Grundrechte\nwichtigste Rechtsgrundlage: EU-Datenschutzgrundverordnung (DSGVO)\nSelbstregulierung: z. B. Partnership on AI (Google, IBM, OpenAI etc.)\nzentrale Herausforderung: KI trifft (teilweise) automatisierte Entscheidungen\n\n\n\nGrundrechte laut EU-Charta (Auswahl)\n\nWürde (Art. 1) – besonders relevant im Gesundheitswesen\n\nKI in Pflege, Diagnose, Prävention: Chancen vs. Persönlichkeitsrecht\n\nFreiheit (Art. 6–19) – Meinungsfreiheit, Datenschutz, Privatsphäre\n\nTrainingsdaten umfassen oft personenbezogene Daten\n\nGleichheit (Art. 20–26) – Verbot von Diskriminierung\n\nBeispiel: US-Risikoalgorithmus für Rückfallprognosen war rassistisch verzerrt\n\n\n\n\nWeitere Rechte und Risiken\n\nSolidarität (Art. 27–38) – faire Arbeitsbedingungen, soziale Sicherheit\n\nGefahr: Arbeitsplatzverluste durch Automatisierung\nChance: Umweltschutz durch KI, z. B. 40% weniger Energieverbrauch bei Google\n\nBürgerrechte (Art. 39–46) – z. B. Wahlrecht\n\nGefahr: gezielte Desinformation durch KI-gestützte Systeme (EU-Wahl 2019)\n\nJustizielle Rechte (Art. 47–50) – faires Verfahren, Verteidigung\n\nKI-Systeme mit Bias können faire Verfahren gefährden",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#begriffe-klausurrelevant",
    "href": "include/01_01_einfuehrung_kuenstliche_intelligenz.html#begriffe-klausurrelevant",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Begriffe: Klausurrelevant",
    "text": "Begriffe: Klausurrelevant\n\ndie wichtigsten Begriffe und Definitionen aus Abschnitt 1.1.3\nsind klausurrelevant und müssen ``auswendig’’ gelernt werden\n\na.: Artificial Neural Network, Deep Learning, Overfitting, Accuracy, Dataset, Hyperparameter\n\nbitte eigenständig anhand des Studienbriefs wiederholen\n\n\nKlausur: Beispielbegriffe die bisher abgefragt wurden\n\nDeep learning: Eine Funktion der künstlichen Intelligenz, die das menschliche Gehirn imitiert, indem sie aus der Art und Weise lernt, wie Daten strukturiert sind, und nicht aus einem Algorithmus, der auf eine bestimmte Aufgabe programmiert ist.\nNatural language processing (NLP): Der Oberbegriff für die Fähigkeit einer Maschine, Konversationsaufgaben zu erfüllen, z. B. zu erkennen, was zu ihr gesagt wird, die beabsichtigte Bedeutung zu verstehen und verständlich zu antworten.\n\nACHTUNG: es liegt nahe Kapitel 1.1.3 zu fokussieren, da Begriffe bisher immer aus diesem Teil des Skripts abgefragt wurden – aber natürlich keine Garantie für zukünftige Klausuren, nur ein gutes Indiz !",
    "crumbs": [
      "Einführung in die Künstliche Intelligenz",
      "Einführung in die Künstliche Intelligenz"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html",
    "href": "include/02_04_MLP.html",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Lineare Modelle wie das Perzeptron sind begrenzt\n\nKönnen nur linear separierbare Daten klassifizieren\n\nBeispiel: XOR ist nicht linear trennbar\n\nLösung: Komposition einfacher nichtlinearer Funktionen\n\n\\(\\ra\\) Einführung verdeckter Schichten (Hidden Layer)\n\n\n\n\n\nEingabe: \\(\\bs{x}_i \\in \\mathbb{R}^d\\)\n\nHidden Layer: Dimension \\(h_1\\)\n\nAusgang: \\(\\hat{y}_i \\in (0,1)\\)\n\n\n\n\n\nEingabe \\(\\rightarrow\\) Hidden: \\(\\bs{W}^{(1)} \\in \\mathbb{R}^{h_1 \\times d},\\quad \\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\\)\nHidden \\(\\rightarrow\\) Output: \\(\\bs{W}^{(2)} \\in \\mathbb{R}^{1 \\times h_1},\\quad b^{(2)} \\in \\mathbb{R}\\)\n\n\n\n\n\nHidden Layer: \\(\\bs{a}_i^{(1)} = \\bs{W}^{(1)} \\bs{x}_i + \\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\\) und \\(\\bs{h}_i = \\phi\\left( \\bs{a}_i^{(1)} \\right) \\in \\mathbb{R}^{h_1}\\)\nOutput Layer: \\(z_i^{(2)} = \\bs{W}^{(2)} \\bs{h}_i + b^{(2)} \\in \\mathbb{R}\\) und \\(\\hat{y}_i = \\sigma(z_i^{(2)}) \\in (0,1)\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html#motivation-multilayer-perceptron-mlp-und-kernidee",
    "href": "include/02_04_MLP.html#motivation-multilayer-perceptron-mlp-und-kernidee",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "",
    "text": "Lineare Modelle wie das Perzeptron sind begrenzt\n\nKönnen nur linear separierbare Daten klassifizieren\n\nBeispiel: XOR ist nicht linear trennbar\n\nLösung: Komposition einfacher nichtlinearer Funktionen\n\n\\(\\ra\\) Einführung verdeckter Schichten (Hidden Layer)\n\n\n\n\n\nEingabe: \\(\\bs{x}_i \\in \\mathbb{R}^d\\)\n\nHidden Layer: Dimension \\(h_1\\)\n\nAusgang: \\(\\hat{y}_i \\in (0,1)\\)\n\n\n\n\n\nEingabe \\(\\rightarrow\\) Hidden: \\(\\bs{W}^{(1)} \\in \\mathbb{R}^{h_1 \\times d},\\quad \\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\\)\nHidden \\(\\rightarrow\\) Output: \\(\\bs{W}^{(2)} \\in \\mathbb{R}^{1 \\times h_1},\\quad b^{(2)} \\in \\mathbb{R}\\)\n\n\n\n\n\nHidden Layer: \\(\\bs{a}_i^{(1)} = \\bs{W}^{(1)} \\bs{x}_i + \\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\\) und \\(\\bs{h}_i = \\phi\\left( \\bs{a}_i^{(1)} \\right) \\in \\mathbb{R}^{h_1}\\)\nOutput Layer: \\(z_i^{(2)} = \\bs{W}^{(2)} \\bs{h}_i + b^{(2)} \\in \\mathbb{R}\\) und \\(\\hat{y}_i = \\sigma(z_i^{(2)}) \\in (0,1)\\)",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html#modelaufbau",
    "href": "include/02_04_MLP.html#modelaufbau",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Modelaufbau",
    "text": "Modelaufbau\n\nKomponenten des Modells\n\n\\(\\phi\\): Aktivierungsfunktion im Hidden Layer (z. B. ReLU, \\(\\tanh\\), Sigmoid)\n\n\\(\\sigma\\): Aktivierung im Output Layer (typisch Sigmoid bei binärer Klassifikation)\n\n\n\nDimensionsüberprüfung\n\n\\(\\bs{x}_i \\in \\mathbb{R}^d\\)\n\n\\(\\bs{W}^{(1)} \\bs{x}_i \\in \\mathbb{R}^{h_1}\\)\n\n\\(\\bs{h}_i \\in \\mathbb{R}^{h_1}\\)\n\n\\(\\hat{y}_i = \\sigma(\\bs{W}^{(2)} \\bs{h}_i + b^{(2)}) \\in (0,1)\\)\n\n\n\nBeispiel\n\nBeispielhafte Netzstruktur:\n\\[\n\\bs{x}_i \\rightarrow \\text{Linear}(\\bs{W}^{(1)}) \\rightarrow \\phi()\n\\rightarrow \\text{Linear}(\\bs{W}^{(2)}) \\rightarrow \\sigma() \\rightarrow \\hat{y}_i\n\\]\nAnzahl der Parameter:\n\\[\nh_1 \\cdot d + h_1 + h_1 + 1 = h_1(d + 2) + 1\n\\]\nDoppelt \\(h_1\\): Bias im Hidden Layer und Gewichte im Output\n\n\n\nKlarstellung: Warum \\(\\bs{W}^{(2)} \\in \\mathbb{R}^{1 \\times h_1}\\)?\n\nDie Form von \\(\\bs{W}^{(2)}\\) hängt nicht von der Aktivierungsfunktion \\(\\phi\\) ab\n\n\\(\\phi\\) wirkt elementweise auf Vektoren in \\(\\mathbb{R}^{h_1}\\)\n\n\\(\\phi\\) verlangt keine Skalareingabe im Output-Layer\n\n\n\nKlarstellung: Outputstruktur bestimmt Form von \\(\\bs{W}^{(2)}\\)\n\nZiel: skalare Ausgabe \\(\\hat{y}_i \\in (0,1)\\)\n\nDazu: \\(\\bs{W}^{(2)}\\) muss \\(\\bs{h}_i \\in \\mathbb{R}^{h_1}\\) zu Skalar kombinieren:\n\\[\nz_i^{(2)} = \\bs{W}^{(2)} \\bs{h}_i + b^{(2)} \\in \\mathbb{R}\n\\]\nDeshalb: \\(\\bs{W}^{(2)} \\in \\mathbb{R}^{1 \\times h_1}\\)\n\n\n\nAchtung: Erweiterung auf \\(C\\) Klassen (Softmax)\n\nBei Klassifikation mit \\(C\\) Klassen:\n\nOutput ist Vektor \\(\\hat{\\bs{y}}_i \\in \\mathbb{R}^C\\)\nGewichtsmatrix: \\(\\bs{W}^{(2)} \\in \\mathbb{R}^{C \\times h_1}\\)\n\nSoftmax erzeugt Wahrscheinlichkeitsverteilung über \\(C\\) Klassen\n\nFazit: Form von \\(\\bs{W}^{(2)}\\) ergibt sich aus gewünschter Ausgabedimension",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html#vorwärtspropagation-im-mlp",
    "href": "include/02_04_MLP.html#vorwärtspropagation-im-mlp",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Vorwärtspropagation im MLP",
    "text": "Vorwärtspropagation im MLP\n\nZiel\n\nVollständige Vorwärtspropagation im MLP formal darstellen\n\nNotation:\n\n\\(\\bs{x}_i \\in \\mathbb{R}^d\\): Eingabevektor\n\n\\(h_1\\): Dimension des Hidden Layers\n\n\\(\\phi\\): Aktivierung im Hidden Layer\n\n\\(\\sigma\\): Aktivierung im Output Layer\n\n\n\n\nGewichtsmatrizen und Biasvektoren\n\n\\(\\bs{W}^{(1)} \\in \\mathbb{R}^{h_1 \\times d}\\), \\(\\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\\)\n\n\\(\\bs{W}^{(2)} \\in \\mathbb{R}^{1 \\times h_1}\\), \\(b^{(2)} \\in \\mathbb{R}\\)\n\n\n\nVorwärtspass: Hidden Layer\n\nAffiner Schritt:\n\\[\n\\bs{a}_i^{(1)} = \\bs{W}^{(1)} \\bs{x}_i + \\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\n\\]\nAktivierung (elementweise):\n\\[\n\\bs{h}_i = \\phi\\left( \\bs{a}_i^{(1)} \\right) \\in \\mathbb{R}^{h_1}\n\\]\n\n\n\nVorwärtspass: Output Layer\n\nAffiner Schritt:\n\\[\nz_i^{(2)} = \\bs{W}^{(2)} \\bs{h}_i + b^{(2)} \\in \\mathbb{R}\n\\]\nAktivierung:\n\\[\n\\hat{y}_i = \\sigma(z_i^{(2)}) \\in (0,1)\n\\]\n\n\n\nGesamtmodell als Funktion\n\nKomposition der Schritte:\n\\[\n\\hat{y}_i = \\sigma\\left( \\bs{W}^{(2)} \\cdot \\phi\\left( \\bs{W}^{(1)} \\bs{x}_i + \\bs{b}^{(1)} \\right) + b^{(2)} \\right)\n\\]\nFunktion \\(f_\\theta: \\mathbb{R}^d \\ra (0,1)\\) mit Parametern:\n\\[\n\\theta = \\left\\{ \\bs{W}^{(1)}, \\bs{b}^{(1)}, \\bs{W}^{(2)}, b^{(2)} \\right\\}\n\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html#vorwärtspropagation-im-mlp-beispiel",
    "href": "include/02_04_MLP.html#vorwärtspropagation-im-mlp-beispiel",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Vorwärtspropagation im MLP: Beispiel",
    "text": "Vorwärtspropagation im MLP: Beispiel\n\nBeispiel: Hidden Layer\n\n\\(d = 2\\), \\(h_1 = 3\\)\n\n\\(\\bs{x}_i = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\)\n\n\\(\\bs{W}^{(1)} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix}\\),\n\\(\\bs{b}^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\)\n\nAktivierung: \\(\\phi = \\text{ReLU}\\)\n\n\\[\n\\bs{a}_i^{(1)} =\n\\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n+\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n\\]\n\\[\n\\bs{h}_i = \\text{ReLU}\\left( \\bs{a}_i^{(1)} \\right) =\n\\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n\\]\n\n\nBeispiel: Output Layer\n\n\\(\\bs{W}^{(2)} = \\begin{pmatrix} 1 & -1 & 2 \\end{pmatrix}\\), \\(b^{(2)} = 0\\)\n\n\\[\nz_i^{(2)} =\n\\begin{pmatrix} 1 & -1 & 2 \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n= 1 \\cdot 1 + (-1) \\cdot 2 + 2 \\cdot 0 = -1\n\\]\n\\[\n\\hat{y}_i = \\sigma(-1) \\approx 0.2689\n\\]\n\n\nInterpretation\n\nHidden Layer \\(\\ra\\) nichtlineare Repräsentation der Eingabe\n\nOutput Layer \\(\\ra\\) linearer Schritt + Sigmoid \\(\\ra\\) Wahrscheinlichkeit\n\nGesamtmodell ist vollständig differenzierbar",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html#backpropagation",
    "href": "include/02_04_MLP.html#backpropagation",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nZiel\n\nEffizienter Gradient der Verlustfunktion bezüglich aller Modellparameter:\n\\[\n\\theta = \\left\\{ \\bs{W}^{(1)}, \\bs{b}^{(1)}, \\bs{W}^{(2)}, b^{(2)} \\right\\}\n\\]\nZentrale Methode: Backpropagation basierend auf Kettenregel der Ableitung\n\n\n\nWiederholung: Vorwärtspass\n\\[\n\\bs{a}_i^{(1)} = \\bs{W}^{(1)} \\bs{x}_i + \\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\n\\] \\[\n\\bs{h}_i = \\phi\\left( \\bs{a}_i^{(1)} \\right) \\in \\mathbb{R}^{h_1}\n\\] \\[\nz_i^{(2)} = \\bs{W}^{(2)} \\bs{h}_i + b^{(2)} \\in \\mathbb{R}\n\\] \\[\n\\hat{y}_i = \\sigma(z_i^{(2)}) \\in (0,1)\n\\]\n\\[\n\\mathcal{L}_i = -\\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\n\n\nSchritt 1: Ableitung am Output\n\nGesucht:\n\\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i^{(2)}}\n\\]\nHerleitung:\n\n\\(\\mathcal{L}_i\\) hängt von \\(\\hat{y}_i = \\sigma(z_i^{(2)})\\) ab\n\n\\(\\frac{d}{dz} \\mathcal{L}_i = (\\sigma(z_i^{(2)}) - y_i) \\cdot \\sigma'(z_i^{(2)})\\)\n\nmit \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\)\n\nErgibt: \\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i^{(2)}} = \\hat{y}_i - y_i\n\\]\n\n\n\nSchritt 2: Gradienten Output Layer\n\nFür \\(\\bs{W}^{(2)} \\in \\mathbb{R}^{1 \\times h_1}\\): \\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial \\bs{W}^{(2)}} = (\\hat{y}_i - y_i) \\cdot \\bs{h}_i^\\top\n\\]\nFür \\(b^{(2)} \\in \\mathbb{R}\\): \\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial b^{(2)}} = \\hat{y}_i - y_i\n\\]\n\n\n\nSchritt 3: Fehlerterm im Hidden Layer\n\nFehlerterm wird rückwärts weitergegeben: \\[\n\\bs{\\delta}_i^{(1)} = (\\hat{y}_i - y_i) \\cdot \\bs{W}^{(2)\\top} \\circ \\phi'\\left( \\bs{a}_i^{(1)} \\right)\n\\]\n\\(\\circ\\): Hadamard-Produkt (elementweise Multiplikation)\n\n\\(\\phi'(\\bs{a}_i^{(1)})\\): Ableitung der Aktivierungsfunktion pro Neuron\n\n\n\nSchritt 4: Gradienten Hidden Layer\n\nFür \\(\\bs{W}^{(1)} \\in \\mathbb{R}^{h_1 \\times d}\\): \\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial \\bs{W}^{(1)}} = \\bs{\\delta}_i^{(1)} \\cdot \\bs{x}_i^\\top\n\\]\nFür \\(\\bs{b}^{(1)} \\in \\mathbb{R}^{h_1}\\): \\[\n\\frac{\\partial \\mathcal{L}_i}{\\partial \\bs{b}^{(1)}} = \\bs{\\delta}_i^{(1)}\n\\]\n\n\n\nGesamtalgorithmus: Backpropagation\n\nFür jedes \\(i = 1, \\dots, N\\):\n\nVorwärtspass: berechne \\(\\hat{y}_i\\)\n\nFehlerterm: \\(\\delta_i^{(2)} = \\hat{y}_i - y_i\\)\n\nRückpropagation: berechne \\(\\bs{\\delta}_i^{(1)}\\)\n\nGradienten für alle Gewichte und Bias berechnen\n\nSGD-Update: \\[\n\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}_i\n\\]",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  },
  {
    "objectID": "include/02_04_MLP.html#training-und-praktische-aspekte",
    "href": "include/02_04_MLP.html#training-und-praktische-aspekte",
    "title": "Grundlagen der Künstlichen Intelligenz (GKI)",
    "section": "Training und praktische Aspekte",
    "text": "Training und praktische Aspekte\n\nWahl von Lernrate und Initialisierung\n\nZu große Lernrate \\(\\eta\\) \\(\\rightarrow\\) Divergenz\n\nZu kleine Lernrate \\(\\rightarrow\\) langsames Lernen\n\nHäufig: Start mit moderatem \\(\\eta\\) und Reduktion über Epochen\n\nInitialisierung der Gewichte:\n\nKleine Zufallswerte (z. B. Normalverteilung)\n\nVermeidung von Symmetrie: nicht alle Gewichte = 0\n\n\n\n\nOverfitting und Generalisierung\n\nZu hohe Modellkapazität \\(\\rightarrow\\) Anpassung an Rauschen in den Trainingsdaten\n\nSymptome:\n\nTraining loss sinkt weiter, Validation loss steigt\n\nKlassifikationsfehler im Test höher als erwartet\n\nLösung: Regularisierung oder Early Stopping\n\n\n\nMini-Batch Training und SGD\n\nSGD: Gewichtsupdate nach jedem Datenpunkt\n\nBatch Gradient Descent: über alle Daten summieren\n\nMini-Batch SGD:\n\nReduzierter Rechenaufwand pro Schritt\n\nStabilerer Verlauf durch Mittelung über kleine Teilmengen\n\n\nTypische Batchgrößen: \\(B = 32, 64, 128\\)\n\n\n\nRegularisierung\n\nL2-Regularisierung (Ridge): \\[\n\\mathcal{L}_\\text{reg} = \\mathcal{L} + \\lambda \\|\\bs{W}\\|_2^2\n\\]\nKontrolliert die Größe der Gewichte, verhindert Überanpassung\nDropout:\n\nZufälliges Deaktivieren von Neuronen während des Trainings\n\nErhöht Robustheit, reduziert Abhängigkeit einzelner Pfade\n\n\n\n\nGrenzen klassischer MLPs\n\nKeine Gedächtnisstruktur \\(\\rightarrow\\) nicht geeignet für zeitabhängige Daten\n\nEingaben müssen fester Länge sein \\(\\rightarrow\\) keine sequenzielle Verarbeitung\n\nTraining bei tieferen Architekturen problematisch:\n\nVanishing Gradient\n\nRechenaufwand steigt stark\n\nDeshalb: CNNs für Bilddaten, RNNs/LSTMs für Sequenzen",
    "crumbs": [
      "Zentrale Algorithmen der künstlichen Intelligenz",
      "Neuronale Netze II: MLP"
    ]
  }
]